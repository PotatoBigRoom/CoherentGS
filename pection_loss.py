#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import time
import math
from typing import Optional, Tuple, List

def _load_vgg16_pretrained() -> nn.Module:
    """å…¼å®¹æ–°æ—§ torchvision çš„ VGG16 é¢„è®­ç»ƒæƒé‡åŠ è½½ã€‚"""
    try:
        # torchvision >= 0.13
        from torchvision.models import VGG16_Weights
        vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)
    except Exception:
        # æ—§ç‰ˆæœ¬
        vgg = models.vgg16(pretrained=True)
    return vgg

class VGG16PerceptualLoss(nn.Module):
    """
    VGG-16 Perceptual Loss
    ä½¿ç”¨VGG-16çš„æŸå±‚ç‰¹å¾è®¡ç®—æ„ŸçŸ¥æŸå¤±
    """

    def __init__(
        self,
        feature_layer: str = 'relu2_2',
        normalize: bool = True,
        resize_input: bool = True,
        requires_grad: bool = False,
        device: Optional[str] = 'cuda',     # å¼ºåˆ¶ä½¿ç”¨ GPUï¼›è‹¥æƒ³è‡ªåŠ¨ï¼Œå¯è®¾ä¸º None
        dtype: torch.dtype = torch.float32, # ç»Ÿä¸€ dtype
        enable_timing: bool = True,         # æ˜¯å¦å¯ç”¨æ—¶é—´ç»Ÿè®¡
    ):
        """
        Args:
            feature_layer: ç›®æ ‡ç‰¹å¾å±‚ ('relu1_2','relu2_2','relu3_3','relu4_3','relu5_1')
            normalize: æ˜¯å¦åš ImageNet æ ‡å‡†åŒ–
            resize_input: æ˜¯å¦ç¼©æ”¾åˆ° 224x224
            requires_grad: VGG ç‰¹å¾æ˜¯å¦å‚ä¸åä¼ 
            device: è®¡ç®—è®¾å¤‡ï¼›é»˜è®¤ 'cuda'ã€‚è‹¥ä¸º None ä¸”æœ‰ CUDAï¼Œåˆ™è‡ªåŠ¨ç”¨ 'cuda's
            dtype: è®¡ç®—ç²¾åº¦ï¼ˆé»˜è®¤ float32ï¼‰
            enable_timing: æ˜¯å¦å¯ç”¨æ—¶é—´ç»Ÿè®¡
        """
        super().__init__()

        # è®¾å¤‡é€‰æ‹©
        if device is None:
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        if device.startswith('cuda') and not torch.cuda.is_available():
            raise RuntimeError("è¦æ±‚åœ¨ GPU ä¸Šè¿è¡Œï¼Œä½†å½“å‰ç¯å¢ƒæœªæ£€æµ‹åˆ° CUDAã€‚")

        self.device = torch.device(device)
        self.dtype = dtype
        self.enable_timing = enable_timing

        # æ”¯æŒçš„å±‚
        self.feature_layers = {
            'relu1_2': 3,   # features[0..4]
            'relu2_2': 8,   # features[0..9]
            'relu3_3': 15,  # features[0..16]
            'relu4_3': 22,  # features[0..23]
            'relu5_1': 25,  # features[0..26]
        }
        if feature_layer not in self.feature_layers:
            raise ValueError(f"ä¸æ”¯æŒçš„ç‰¹å¾å±‚: {feature_layer}. æ”¯æŒ: {list(self.feature_layers.keys())}")

        self.feature_layer_idx = self.feature_layers[feature_layer]
        self.feature_layer_name = feature_layer
        
        # åŠ è½½ VGG16 å¹¶æ„é€ ç‰¹å¾æå–å™¨
        vgg = _load_vgg16_pretrained()
        self.feature_extractor = nn.Sequential(*list(vgg.features.children())[: self.feature_layer_idx + 1])

        # å†»ç»“ä¸è®­ç»ƒæ¨¡å¼
        for p in self.feature_extractor.parameters():
            p.requires_grad = requires_grad
        self.feature_extractor.eval()

        # æ ‡å‡†åŒ–å‚æ•°ï¼ˆä½œä¸º bufferï¼Œéš .to(device) ç§»åŠ¨ï¼‰
        self.normalize = normalize
        if normalize:
            self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406], dtype=self.dtype).view(1, 3, 1, 1))
            self.register_buffer('std',  torch.tensor([0.229, 0.224, 0.225], dtype=self.dtype).view(1, 3, 1, 1))
        else:
            # ä¹Ÿæ³¨å†Œå ä½ï¼Œä¾¿äºç±»å‹ä¸€è‡´
            self.register_buffer('mean', torch.zeros(1, 3, 1, 1, dtype=self.dtype))
            self.register_buffer('std',  torch.ones(1, 3, 1, 1, dtype=self.dtype))

        self.resize_input = resize_input

        # æŠŠæ•´ä¸ªæ¨¡å—ï¼ˆå‚æ•°+bufferï¼‰æ¬åˆ°è®¾å¤‡/ç±»å‹
        self.to(self.device, dtype=self.dtype)

        # è·å–è¾“å‡ºé€šé“/å°ºå¯¸ä¿¡æ¯
        self._get_layer_info()

    @torch.no_grad()
    def _get_layer_info(self):
        """è·å–ç›®æ ‡å±‚çš„é€šé“æ•°/å°ºå¯¸ä¿¡æ¯ï¼ˆåœ¨ self.device ä¸Šæ±‚ä¸€æ¬¡å‰å‘ï¼‰"""
        test_input = torch.randn(1, 3, 224, 224, device=self.device, dtype=self.dtype)
        x = test_input
        if self.normalize:
            x = (x - self.mean) / self.std
        features = self.feature_extractor(x)
        self.C_j = features.shape[1]
        self.H_j = features.shape[2]
        self.W_j = features.shape[3]
        # å¯æŒ‰éœ€æ‰“å°
        # print(f"[{self.feature_layer_name}] C:{self.C_j} H:{self.H_j} W:{self.W_j}")

    def preprocess_input(self, x: torch.Tensor) -> torch.Tensor:
        """
        é¢„å¤„ç†è¾“å…¥åˆ° [B, C, H, W]ï¼Œæ”¾åˆ°åŒä¸€ device/dtypeï¼ŒæŒ‰éœ€ resize/normalize
        """
        # ç§»åŠ¨è®¾å¤‡/ç±»å‹
        x = x.to(self.device, dtype=self.dtype, non_blocking=True)

        # ä¿è¯é€šé“ç»´åœ¨å‰
        if x.ndim == 4 and x.shape[-1] == 3:  # [B, H, W, C] -> [B, C, H, W]
            x = x.permute(0, 3, 1, 2).contiguous()

        # å½’ä¸€åŒ–åˆ° [0,1]ï¼ˆè‹¥çœ‹èµ·æ¥åƒ 0..255ï¼‰
        if x.max() > 1.0:
            x = x / 255.0

        # å°ºå¯¸åˆ° 224x224
        if self.resize_input and (x.shape[2] != 224 or x.shape[3] != 224):
            x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)

        # ImageNet æ ‡å‡†åŒ–
        if self.normalize:
            x = (x - self.mean) / self.std

        return x

    def extract_features(self, x: torch.Tensor) -> torch.Tensor:
        """æå– VGG ç‰¹å¾ï¼ˆåœ¨ self.device ä¸Šï¼‰"""
        if self.enable_timing:
            feature_start_time = time.time()
        
        x = self.preprocess_input(x)
        
        if self.enable_timing:
            preprocess_time = time.time() - feature_start_time
            vgg_start_time = time.time()
        
        # å¦‚æœä¸éœ€è¦å¯¹ VGG æ±‚æ¢¯åº¦ï¼Œå¯ä»¥ç”¨ no_grad æé€Ÿ/çœæ˜¾å­˜
        if not any(p.requires_grad for p in self.feature_extractor.parameters()):
            with torch.no_grad():
                features = self.feature_extractor(x)
        else:
            features = self.feature_extractor(x)
        
        if self.enable_timing:
            vgg_time = time.time() - vgg_start_time
            total_feature_time = time.time() - feature_start_time
            print(f"ğŸ” VGGç‰¹å¾æå–æ—¶é—´ç»Ÿè®¡:")
            print(f"   é¢„å¤„ç†æ—¶é—´: {preprocess_time:.4f}s")
            print(f"   VGGæ¨ç†æ—¶é—´: {vgg_time:.4f}s")
            print(f"   æ€»ç‰¹å¾æå–æ—¶é—´: {total_feature_time:.4f}s")
        
        return features

    def forward(
        self,
        pred: torch.Tensor,
        target: torch.Tensor,
        return_features: bool = False
    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        """
        è®¡ç®— perceptual lossï¼š L2(Ï†(pred) - Ï†(target)) / (C*H*W)
        """
        if self.enable_timing:
            forward_start_time = time.time()
        
        # ç»Ÿä¸€è®¾å¤‡/ç±»å‹
        pred = pred.to(self.device, dtype=self.dtype, non_blocking=True)
        target = target.to(self.device, dtype=self.dtype, non_blocking=True)

        # æå–ç‰¹å¾
        pred_features = self.extract_features(pred)
        target_features = self.extract_features(target)

        if self.enable_timing:
            feature_time = time.time() - forward_start_time
            loss_calc_start_time = time.time()

        # è®¡ç®—Loss
        diff = pred_features - target_features
        # [B]
        l2_sq = torch.sum(diff * diff, dim=(1, 2, 3))
        norm = float(self.C_j * self.H_j * self.W_j)
        loss = torch.mean(l2_sq / norm)
        
        if self.enable_timing:
            loss_calc_time = time.time() - loss_calc_start_time
            total_forward_time = time.time() - forward_start_time
            print(f"ğŸ” Perceptual Lossè®¡ç®—æ—¶é—´ç»Ÿè®¡:")
            print(f"   ç‰¹å¾æå–æ—¶é—´: {feature_time:.4f}s")
            print(f"   Lossè®¡ç®—æ—¶é—´: {loss_calc_time:.4f}s")
            print(f"   æ€»å‰å‘æ—¶é—´: {total_forward_time:.4f}s")
            print(f"   ç‰¹å¾å½¢çŠ¶: {pred_features.shape}")
    
        if return_features:
            return loss, (pred_features, target_features)
        return loss

    def get_feature_maps(self, x: torch.Tensor) -> torch.Tensor:
        return self.extract_features(x)

    def get_timing_stats(self) -> dict:
        """è·å–æ—¶é—´ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
        if not self.enable_timing:
            return {"timing_enabled": False}
        
        return {
            "timing_enabled": True,
            "feature_layer": self.feature_layer_name,
            "device": str(self.device),
            "dtype": str(self.dtype)
        }

class VGG16PerceptualLossWithMultipleLayers(nn.Module):
    """
    åŒæ—¶ä½¿ç”¨å¤šä¸ª VGG å±‚çš„ Perceptual Lossï¼ˆå…¨éƒ¨åœ¨åŒä¸€ GPU ä¸Šï¼‰
    """
    def __init__(
        self,
        feature_layers: List[str] = ['relu1_2', 'relu2_2', 'relu3_3'],
        weights: Optional[List[float]] = None,
        normalize: bool = True,
        resize_input: bool = True,
        requires_grad: bool = False,
        device: Optional[str] = 'cuda',
        dtype: torch.dtype = torch.float32,
        enable_timing: bool = True,
    ):
        super().__init__()
        self.feature_layers = feature_layers
        self.weights = weights if weights is not None else [1.0] * len(feature_layers)
        if len(self.weights) != len(self.feature_layers):
            raise ValueError("ç‰¹å¾å±‚æ•°é‡ä¸æƒé‡æ•°é‡å¿…é¡»ä¸€è‡´")

        # åœ¨åŒä¸€ device/dtype ä¸Šæ„å»ºæ‰€æœ‰å­æŸå¤±
        self.loss_modules = nn.ModuleList([
            VGG16PerceptualLoss(
                feature_layer=layer,
                normalize=normalize,
                resize_input=resize_input,
                requires_grad=requires_grad,
                device=device,
                dtype=dtype,
                enable_timing=enable_timing,
            ) for layer in feature_layers
        ])

        # è®°å½•è®¾å¤‡
        if device is None:
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.device = torch.device(device)
        self.dtype = dtype
        self.enable_timing = enable_timing
        self.to(self.device, dtype=self.dtype)

    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        if self.enable_timing:
            multi_start_time = time.time()
            print(f"ğŸ” å¤šå±‚çº§Perceptual Losså¼€å§‹è®¡ç®—...")
        
        pred = pred.to(self.device, dtype=self.dtype, non_blocking=True)
        target = target.to(self.device, dtype=self.dtype, non_blocking=True)

        total = 0.0
        for i, (loss_module, w) in enumerate(zip(self.loss_modules, self.weights)):
            if self.enable_timing:
                layer_start_time = time.time()
                print(f"   å±‚çº§ {i+1}: {loss_module.feature_layer_name}")
            
            layer_loss = loss_module(pred, target)
            total = total + w * layer_loss
            
            if self.enable_timing:
                layer_time = time.time() - layer_start_time
                print(f"   å±‚çº§ {i+1} è€—æ—¶: {layer_time:.4f}s, æƒé‡: {w}, Loss: {layer_loss.item():.6f}")
        
        if self.enable_timing:
            total_time = time.time() - multi_start_time
            print(f"ğŸ” å¤šå±‚çº§æ€»è€—æ—¶: {total_time:.4f}s")
        
        return total

class VGG16DISTSLoss(nn.Module):
    """
    åŸºäºVGG16çš„DISTS (Deep Image Structure and Texture Similarity) Loss
    ä½¿ç”¨relu2_2å’Œrelu3_3å±‚ç‰¹å¾ï¼ŒæŒ‰ç…§ç”¨æˆ·æä¾›çš„å…¬å¼å®ç°
    
    è®¾ I=æ¸²æŸ“å›¾ã€T=ç”Ÿæˆå›¾ï¼ˆè½¯GTï¼‰
    F^(2) = relu2_2(I), G^(2) = relu2_2(T)
    F^(3) = relu3_3(I), G^(3) = relu3_3(T)
    
    å¯¹æ¯å±‚ lâˆˆ{2,3} å’Œæ¯ä¸ªé€šé“ c åœ¨ç©ºé—´ç»´è®¡ç®—:
    - å‡å€¼: Î¼_F,l,c, Î¼_G,l,c
    - æ ‡å‡†å·®: Ïƒ_F,l,c, Ïƒ_G,l,c  
    - åæ–¹å·®: Ïƒ_FG,l,c
    
    çº¹ç†/äº®åº¦ç›¸ä¼¼ï¼ˆ"l"é¡¹ï¼‰:
    l_l = (1/C) * Î£_c [2*Î¼_F,l,c*Î¼_G,l,c + c1] / [Î¼_F,l,cÂ² + Î¼_G,l,cÂ² + c1]
    
    ç»“æ„ç›¸ä¼¼ï¼ˆ"s"é¡¹ï¼‰:
    s_l = (1/C) * Î£_c [2*Ïƒ_FG,l,c + c2] / [Ïƒ_F,l,cÂ² + Ïƒ_G,l,cÂ² + c2]
    
    ä¸¤å±‚æ±‡æ€»ä¸ºè·ç¦»:
    L_DISTS = Î£_lâˆˆ{2,3} [Î±_l*(1-s_l) + Î²_l*(1-l_l)]
    """
    
    def __init__(
        self,
        normalize: bool = True,
        resize_input: bool = True,
        requires_grad: bool = False,
        device: Optional[str] = 'cuda',
        dtype: torch.dtype = torch.float32,
        enable_timing: bool = True,
        # çº¹ç†/ç»“æ„æƒé‡ (Î±_l, Î²_l)
        alpha_2: float = 0.5,  # relu2_2å±‚ç»“æ„æƒé‡
        beta_2: float = 0.5,   # relu2_2å±‚çº¹ç†æƒé‡
        alpha_3: float = 0.5,  # relu3_3å±‚ç»“æ„æƒé‡
        beta_3: float = 0.5,   # relu3_3å±‚çº¹ç†æƒé‡
        # å¸¸æ•°
        c1: float = 1e-6,
        c2: float = 1e-6,
    ):
        """
        Args:
            normalize: æ˜¯å¦åš ImageNet æ ‡å‡†åŒ–
            resize_input: æ˜¯å¦ç¼©æ”¾åˆ° 224x224
            requires_grad: VGG ç‰¹å¾æ˜¯å¦å‚ä¸åä¼ 
            device: è®¡ç®—è®¾å¤‡
            dtype: è®¡ç®—ç²¾åº¦
            enable_timing: æ˜¯å¦å¯ç”¨æ—¶é—´ç»Ÿè®¡
            alpha_2, beta_2: relu2_2å±‚çš„ç»“æ„å’Œçº¹ç†æƒé‡
            alpha_3, beta_3: relu3_3å±‚çš„ç»“æ„å’Œçº¹ç†æƒé‡
            c1, c2: ç¨³å®šæ€§å¸¸æ•°
        """
        super().__init__()
        
        # è®¾å¤‡é€‰æ‹©
        if device is None:
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        if device.startswith('cuda') and not torch.cuda.is_available():
            raise RuntimeError("è¦æ±‚åœ¨ GPU ä¸Šè¿è¡Œï¼Œä½†å½“å‰ç¯å¢ƒæœªæ£€æµ‹åˆ° CUDAã€‚")
        
        self.device = torch.device(device)
        self.dtype = dtype
        self.enable_timing = enable_timing
        
        # æƒé‡å‚æ•°
        self.alpha_2 = alpha_2  # relu2_2ç»“æ„æƒé‡
        self.beta_2 = beta_2    # relu2_2çº¹ç†æƒé‡
        self.alpha_3 = alpha_3  # relu3_3ç»“æ„æƒé‡
        self.beta_3 = beta_3    # relu3_3çº¹ç†æƒé‡
        self.c1 = c1
        self.c2 = c2
        
        # åŠ è½½VGG16å¹¶æå–relu2_2å’Œrelu3_3å±‚
        vgg = _load_vgg16_pretrained()
        
        # relu2_2: features[0..8] (ç¬¬9å±‚)
        self.feature_extractor_2_2 = nn.Sequential(*list(vgg.features.children())[:9])
        # relu3_3: features[0..15] (ç¬¬16å±‚)  
        self.feature_extractor_3_3 = nn.Sequential(*list(vgg.features.children())[:16])
        
        # å†»ç»“å‚æ•°
        for p in self.feature_extractor_2_2.parameters():
            p.requires_grad = requires_grad
        for p in self.feature_extractor_3_3.parameters():
            p.requires_grad = requires_grad
            
        self.feature_extractor_2_2.eval()
        self.feature_extractor_3_3.eval()
        
        # æ ‡å‡†åŒ–å‚æ•°
        self.normalize = normalize
        if normalize:
            self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406], dtype=self.dtype).view(1, 3, 1, 1))
            self.register_buffer('std',  torch.tensor([0.229, 0.224, 0.225], dtype=self.dtype).view(1, 3, 1, 1))
        else:
            self.register_buffer('mean', torch.zeros(1, 3, 1, 1, dtype=self.dtype))
            self.register_buffer('std',  torch.ones(1, 3, 1, 1, dtype=self.dtype))
            
        self.resize_input = resize_input
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.to(self.device, dtype=self.dtype)
        
        # è·å–ç‰¹å¾å±‚ä¿¡æ¯
        self._get_layer_info()
    
    @torch.no_grad()
    def _get_layer_info(self):
        """è·å–relu2_2å’Œrelu3_3å±‚çš„é€šé“æ•°å’Œå°ºå¯¸ä¿¡æ¯"""
        test_input = torch.randn(1, 3, 224, 224, device=self.device, dtype=self.dtype)
        x = test_input
        if self.normalize:
            x = (x - self.mean) / self.std
            
        # relu2_2ç‰¹å¾
        features_2_2 = self.feature_extractor_2_2(x)
        self.C_2_2 = features_2_2.shape[1]
        self.H_2_2 = features_2_2.shape[2]
        self.W_2_2 = features_2_2.shape[3]
        
        # relu3_3ç‰¹å¾
        features_3_3 = self.feature_extractor_3_3(x)
        self.C_3_3 = features_3_3.shape[1]
        self.H_3_3 = features_3_3.shape[2]
        self.W_3_3 = features_3_3.shape[3]
        
        print(f"[DISTS] relu2_2: C:{self.C_2_2} H:{self.H_2_2} W:{self.W_2_2}")
        print(f"[DISTS] relu3_3: C:{self.C_3_3} H:{self.H_3_3} W:{self.W_3_3}")
    
    def preprocess_input(self, x: torch.Tensor) -> torch.Tensor:
        """é¢„å¤„ç†è¾“å…¥"""
        # ç§»åŠ¨è®¾å¤‡/ç±»å‹
        x = x.to(self.device, dtype=self.dtype, non_blocking=True)
        
        # ä¿è¯é€šé“ç»´åœ¨å‰
        if x.ndim == 4 and x.shape[-1] == 3:  # [B, H, W, C] -> [B, C, H, W]
            x = x.permute(0, 3, 1, 2).contiguous()
        
        # å½’ä¸€åŒ–åˆ° [0,1]
        if x.max() > 1.0:
            x = x / 255.0
        
        # å°ºå¯¸åˆ° 224x224
        if self.resize_input and (x.shape[2] != 224 or x.shape[3] != 224):
            x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)
        
        # ImageNet æ ‡å‡†åŒ–
        if self.normalize:
            x = (x - self.mean) / self.std
        
        return x
    
    def extract_features(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """æå–relu2_2å’Œrelu3_3ç‰¹å¾"""
        if self.enable_timing:
            feature_start_time = time.time()
        
        x = self.preprocess_input(x)
        
        if self.enable_timing:
            preprocess_time = time.time() - feature_start_time
            vgg_start_time = time.time()
        
        # æå–ç‰¹å¾
        if not any(p.requires_grad for p in self.feature_extractor_2_2.parameters()):
            with torch.no_grad():
                features_2_2 = self.feature_extractor_2_2(x)
                features_3_3 = self.feature_extractor_3_3(x)
        else:
            features_2_2 = self.feature_extractor_2_2(x)
            features_3_3 = self.feature_extractor_3_3(x)
        
        if self.enable_timing:
            vgg_time = time.time() - vgg_start_time
            total_feature_time = time.time() - feature_start_time
            print(f"ğŸ” DISTSç‰¹å¾æå–æ—¶é—´ç»Ÿè®¡:")
            print(f"   é¢„å¤„ç†æ—¶é—´: {preprocess_time:.4f}s")
            print(f"   VGGæ¨ç†æ—¶é—´: {vgg_time:.4f}s")
            print(f"   æ€»ç‰¹å¾æå–æ—¶é—´: {total_feature_time:.4f}s")
        
        return features_2_2, features_3_3
    
    def compute_dists_loss_layer(self, F: torch.Tensor, G: torch.Tensor, 
                                layer_name: str, alpha: float, beta: float) -> torch.Tensor:
        """
        è®¡ç®—å•å±‚çš„DISTSæŸå¤±ï¼ŒæŒ‰ç…§ç”¨æˆ·æä¾›çš„å…¬å¼å®ç°
        
        Args:
            F: æ¸²æŸ“å›¾ç‰¹å¾ [B, C, H, W]
            G: ç”Ÿæˆå›¾ç‰¹å¾ [B, C, H, W]
            layer_name: å±‚åç§°
            alpha: ç»“æ„æƒé‡
            beta: çº¹ç†æƒé‡
        """
        B, C, H, W = F.shape
        
        # å°†ç‰¹å¾å±•å¹³ä¸º [B, C, H*W]
        F_flat = F.view(B, C, -1)  # [B, C, H*W]
        G_flat = G.view(B, C, -1)  # [B, C, H*W]
        
        # è®¡ç®—æ¯ä¸ªé€šé“çš„ç»Ÿè®¡é‡
        # Î¼_F,l,c = mean(F_l,c)
        mu_F = torch.mean(F_flat, dim=2)  # [B, C]
        mu_G = torch.mean(G_flat, dim=2)  # [B, C]
        
        # Ïƒ_F,l,c = std(F_l,c)
        sigma_F = torch.std(F_flat, dim=2)  # [B, C]
        sigma_G = torch.std(G_flat, dim=2)  # [B, C]
        
        # Ïƒ_FG,l,c = cov(F_l,c, G_l,c)
        # cov(x,y) = E[(x-Î¼_x)(y-Î¼_y)] = E[xy] - Î¼_x*Î¼_y
        F_centered = F_flat - mu_F.unsqueeze(2)  # [B, C, H*W]
        G_centered = G_flat - mu_G.unsqueeze(2)  # [B, C, H*W]
        sigma_FG = torch.mean(F_centered * G_centered, dim=2)  # [B, C]
        
        # è®¡ç®—çº¹ç†/äº®åº¦ç›¸ä¼¼æ€§ l_l
        # l_l = (1/C) * Î£_c [2*Î¼_F,l,c*Î¼_G,l,c + c1] / [Î¼_F,l,cÂ² + Î¼_G,l,cÂ² + c1]
        l_l = (2 * mu_F * mu_G + self.c1) / (mu_F**2 + mu_G**2 + self.c1)  # [B, C]
        l_l = torch.mean(l_l, dim=1)  # [B] - å¯¹é€šé“æ±‚å¹³å‡
        
        # è®¡ç®—ç»“æ„ç›¸ä¼¼æ€§ s_l
        # s_l = (1/C) * Î£_c [2*Ïƒ_FG,l,c + c2] / [Ïƒ_F,l,cÂ² + Ïƒ_G,l,cÂ² + c2]
        s_l = (2 * sigma_FG + self.c2) / (sigma_F**2 + sigma_G**2 + self.c2)  # [B, C]
        s_l = torch.mean(s_l, dim=1)  # [B] - å¯¹é€šé“æ±‚å¹³å‡
        
        # è®¡ç®—æŸå¤±é¡¹: Î±_l*(1-s_l) + Î²_l*(1-l_l)
        structure_loss = alpha * (1 - s_l)  # [B]
        texture_loss = beta * (1 - l_l)     # [B]
        
        # å¯¹batchæ±‚å¹³å‡
        layer_loss = torch.mean(structure_loss + texture_loss)  # æ ‡é‡
        
        if self.enable_timing:
            print(f"   {layer_name} - ç»“æ„ç›¸ä¼¼æ€§: {torch.mean(s_l).item():.6f}, çº¹ç†ç›¸ä¼¼æ€§: {torch.mean(l_l).item():.6f}")
            print(f"   {layer_name} - ç»“æ„æŸå¤±: {torch.mean(structure_loss).item():.6f}, çº¹ç†æŸå¤±: {torch.mean(texture_loss).item():.6f}")
        
        return layer_loss
    
    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—DISTSæŸå¤±ï¼ŒæŒ‰ç…§ç”¨æˆ·æä¾›çš„å…¬å¼:
        L_DISTS = Î£_lâˆˆ{2,3} [Î±_l*(1-s_l) + Î²_l*(1-l_l)]
        """
        if self.enable_timing:
            forward_start_time = time.time()
        
        # ç»Ÿä¸€è®¾å¤‡/ç±»å‹
        pred = pred.to(self.device, dtype=self.dtype, non_blocking=True)
        target = target.to(self.device, dtype=self.dtype, non_blocking=True)
        
        # æå–ç‰¹å¾
        F_2, F_3 = self.extract_features(pred)   # æ¸²æŸ“å›¾ç‰¹å¾
        G_2, G_3 = self.extract_features(target) # ç”Ÿæˆå›¾ç‰¹å¾
        
        if self.enable_timing:
            feature_time = time.time() - forward_start_time
            loss_calc_start_time = time.time()
        
        # è®¡ç®—å„å±‚æŸå¤±
        loss_2 = self.compute_dists_loss_layer(F_2, G_2, "relu2_2", self.alpha_2, self.beta_2)
        loss_3 = self.compute_dists_loss_layer(F_3, G_3, "relu3_3", self.alpha_3, self.beta_3)
        
        # ä¸¤å±‚æ±‡æ€»: L_DISTS = loss_2 + loss_3
        total_loss = loss_2 + loss_3
        
        if self.enable_timing:
            loss_calc_time = time.time() - loss_calc_start_time
            total_forward_time = time.time() - forward_start_time
            print(f"ğŸ” DISTS Lossè®¡ç®—æ—¶é—´ç»Ÿè®¡:")
            print(f"   ç‰¹å¾æå–æ—¶é—´: {feature_time:.4f}s")
            print(f"   Lossè®¡ç®—æ—¶é—´: {loss_calc_time:.4f}s")
            print(f"   æ€»å‰å‘æ—¶é—´: {total_forward_time:.4f}s")
            print(f"   relu2_2æŸå¤±: {loss_2.item():.6f}")
            print(f"   relu3_3æŸå¤±: {loss_3.item():.6f}")
            print(f"   æ€»DISTSæŸå¤±: {total_loss.item():.6f}")
        
        return total_loss
    
    def get_feature_maps(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """è·å–relu2_2å’Œrelu3_3ç‰¹å¾å›¾"""
        return self.extract_features(x)


def create_perceptual_loss(
    feature_layer: str = 'relu2_2',
    use_multiple_layers: bool = False,
    use_dists: bool = False,
    enable_timing: bool = True,
    **kwargs
) -> nn.Module:
    """
    åˆ›å»ºæ„ŸçŸ¥æŸå¤±æ¨¡å—
    
    Args:
        feature_layer: å•å±‚æ¨¡å¼ä¸‹çš„ç‰¹å¾å±‚
        use_multiple_layers: æ˜¯å¦ä½¿ç”¨å¤šå±‚çº§æ„ŸçŸ¥æŸå¤±
        use_dists: æ˜¯å¦ä½¿ç”¨DISTSæŸå¤±
        enable_timing: æ˜¯å¦å¯ç”¨æ—¶é—´ç»Ÿè®¡
        **kwargs: å…¶ä»–å‚æ•°
    """
    if use_dists:
        return VGG16DISTSLoss(enable_timing=enable_timing, **kwargs)
    elif use_multiple_layers:
        return VGG16PerceptualLossWithMultipleLayers(enable_timing=enable_timing, **kwargs)
    else:
        return VGG16PerceptualLoss(feature_layer=feature_layer, enable_timing=enable_timing, **kwargs)

if __name__ == "__main__":
    # ==== ä½¿ç”¨ç¤ºä¾‹ï¼ˆå…¨ GPUï¼‰ ====
    device = 'cuda'  # å¼ºåˆ¶ GPU
    dtype = torch.float32
    enable_timing = True  # å¯ç”¨æ—¶é—´ç»Ÿè®¡

    print("=== å•å±‚ Perceptual Loss ç¤ºä¾‹ï¼ˆGPUï¼‰ ===")
    perceptual_loss = VGG16PerceptualLoss(
        feature_layer='relu2_2', 
        device=device, 
        dtype=dtype,
        enable_timing=enable_timing
    )

    B = 2
    pred = torch.randn(B, 3, 256, 256, device=device, dtype=dtype)
    target = torch.randn(B, 3, 256, 256, device=device, dtype=dtype)

    loss = perceptual_loss(pred, target)
    print(f"å•å±‚æŸå¤±å€¼: {loss.item():.6f}")

    print("\n=== å¤šå±‚çº§ Perceptual Loss ç¤ºä¾‹ï¼ˆGPUï¼‰ ===")
    multi_loss_module = VGG16PerceptualLossWithMultipleLayers(
        feature_layers=['relu1_2','relu2_2','relu3_3'],
        weights=[0.1, 1.0, 0.1],
        device=device,
        dtype=dtype,
        enable_timing=enable_timing,
    )
    multi_loss = multi_loss_module(pred, target)
    print(f"å¤šå±‚çº§æŸå¤±å€¼: {multi_loss.item():.6f}")

    print("\n=== DISTS Loss ç¤ºä¾‹ï¼ˆGPUï¼‰ ===")
    dists_loss = VGG16DISTSLoss(
        alpha_2=0.5,  # relu2_2ç»“æ„æƒé‡
        beta_2=0.5,   # relu2_2çº¹ç†æƒé‡
        alpha_3=0.5,  # relu3_3ç»“æ„æƒé‡
        beta_3=0.5,   # relu3_3çº¹ç†æƒé‡
        device=device,
        dtype=dtype,
        enable_timing=enable_timing
    )
    dists_loss_value = dists_loss(pred, target)
    print(f"DISTSæŸå¤±å€¼: {dists_loss_value.item():.6f}")

    print("\n=== ç‰¹å¾æå–æµ‹è¯•ï¼ˆGPUï¼‰ ===")
    feats = perceptual_loss.get_feature_maps(pred)
    print(f"å•å±‚ç‰¹å¾å½¢çŠ¶: {feats.shape}")

    feats_2_2, feats_3_3 = dists_loss.get_feature_maps(pred)
    print(f"DISTS relu2_2ç‰¹å¾å½¢çŠ¶: {feats_2_2.shape}")
    print(f"DISTS relu3_3ç‰¹å¾å½¢çŠ¶: {feats_3_3.shape}")

    loss, (pf, tf) = perceptual_loss(pred, target, return_features=True)
    print(f"æŸå¤±å€¼: {loss.item():.6f} | é¢„æµ‹ç‰¹å¾: {pf.shape} | ç›®æ ‡ç‰¹å¾: {tf.shape}")

    print("\n=== æ—¶é—´ç»Ÿè®¡ä¿¡æ¯ ===")
    timing_stats = perceptual_loss.get_timing_stats()
    for k, v in timing_stats.items():
        print(f"   {k}: {v}")

    print("\n=== ä½¿ç”¨create_perceptual_losså‡½æ•° ===")
    # åˆ›å»ºDISTSæŸå¤±
    dists_loss_2 = create_perceptual_loss(
        use_dists=True,
        alpha_2=0.6,  # relu2_2ç»“æ„æƒé‡
        beta_2=0.4,   # relu2_2çº¹ç†æƒé‡
        alpha_3=0.5,  # relu3_3ç»“æ„æƒé‡
        beta_3=0.5,   # relu3_3çº¹ç†æƒé‡
        device=device,
        enable_timing=enable_timing
    )
    dists_loss_2_value = dists_loss_2(pred, target)
    print(f"é€šè¿‡create_perceptual_lossåˆ›å»ºçš„DISTSæŸå¤±: {dists_loss_2_value.item():.6f}")
