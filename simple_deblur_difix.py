#!/usr/bin/env python3
"""
BAD-Gaussianså»æ¨¡ç³Šè®­ç»ƒå™¨ + DiFix3Dé›†æˆç‰ˆæœ¬

åŸºäºsimple_trainer_deblur.pyï¼Œé›†æˆDiFix3Då›¾åƒå¢å¼ºåŠŸèƒ½
æ”¯æŒè¿åŠ¨æ¨¡ç³Šå»é™¤å’ŒDiFix3Dè”åˆä¼˜åŒ–
é›†æˆSE(3)æ··åˆé‡‡æ ·ç­–ç•¥
"""

import json
import math
import os
import time
import yaml
from collections import defaultdict
from pathlib import Path
from typing import List, Optional, Union

# å¯¼å…¥SE(3)æ’å¸§æ¨¡å—
import sys
from typing_extensions import assert_never

from hybrid_sampling import generate_camera_trajectory,se3_interpolate_to_target
from scoring_model import VirtualViewQualityScorer
import imageio
import numpy as np
import torch
import torch.nn.functional as F
import tqdm
import tyro
import viser
from dataclasses import dataclass, field
from pytorch_msssim import ssim as pytorch_ssim
from gsplat.distributed import cli
from gsplat.strategy import DefaultStrategy, MCMCStrategy
from nerfview.viewer import Viewer
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.tensorboard import SummaryWriter
from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure
from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from mpl_toolkits.mplot3d import Axes3D
from bad_gaussians.bad_camera_optimizer import BadCameraOptimizer, BadCameraOptimizerConfig
from datasets.blender_dataperser import BlenderParser
from datasets.colmap import Dataset
from datasets.colmap_dataparser import ColmapParser
from datasets.deblur_nerf import DeblurNerfDataset
from pose_viewer import PoseViewer
from simple_trainer import Config, Runner, create_splats_with_optimizers
from lib_bilagrid import (
    BilateralGrid,
    slice,
    color_correct,
    total_variation_loss,
)
from utils import (
    AppearanceOptModule,
    CameraOptModuleSE3,
    set_random_seed,
)

# Perceptual Losså¯¼å…¥
from pection_loss import VGG16PerceptualLoss, VGG16PerceptualLossWithMultipleLayers, VGG16DISTSLoss

# DiFix3Dé›†æˆï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å—ï¼Œä¸æ³¨å…¥ç»å¯¹è·¯å¾„ï¼‰
from PIL import Image
try:
    # ç›´æ¥ä»åŒç›®å½•ä¸‹çš„ pipeline_difix.py å¯¼å…¥
    from pipeline_difix import DifixPipeline
    DIFIX3D_AVAILABLE = True
except Exception as e:
    DIFIX3D_AVAILABLE = False
    print(f"è­¦å‘Š: æœªæ‰¾åˆ°æœ¬åœ° pipeline_difix æˆ–ä¾èµ–ç¼ºå¤±ï¼Œå°†ç¦ç”¨DiFix3DåŠŸèƒ½: {e}")
    

@dataclass
class DeblurDiFix3DConfig(Config):
    """BAD-Gaussianså»æ¨¡ç³Š + DiFix3Dé…ç½®"""
    
    # æ•°æ®é…ç½®
    data_dir: str = "/remote-home/fcr/Event_proj/DeblurDIFIXZK/BAD-Gaussians-gsplat-only_vgg3/data/bad-nerf-gtK-colmap-nvs/blurpool"
    data_factor: int = 1
    # æŒ‡å®šè®­ç»ƒé›†å›¾åƒIDåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰ï¼›None è¡¨ç¤ºä½¿ç”¨é»˜è®¤çš„å…¨éƒ¨è®­ç»ƒè§†è§’
    train_indices: Optional[List[int]] = None
    
    # è¯„ä¼°é…ç½®s
    eval_only: bool = False
    """æ˜¯å¦ä»…æ‰§è¡Œè¯„ä¼°"""
    eval_steps: List[int] = field(default_factory=lambda: [3_000, 7_000])
    """è¯„ä¼°æ­¥éª¤åˆ—è¡¨"""
    scale_factor: float = 1.0
    result_dir: str = "/remote-home/fcr/Event_proj/DeblurDIFIXZK/results"
    test_every: int = 8

    ########### Viewer ###############
    disable_viewer: bool = False
    port: int = 8080
    visualize_cameras: bool = True

    ########### Training ###############
    max_steps: int = 30000
    eval_steps: List[int] = field(default_factory=lambda: [3_000, 7_000, 10_000, 15_000, 20_000, 25_001,30_000])
    save_steps: List[int] = field(default_factory=lambda: [3_000, 7_000, 10_000, 15_000, 20_000,25_001, 30_000])
    
    # ä½¿ç”¨fused SSIMä¼˜åŒ–
    fused_ssim: bool = False
    pin_memory: bool = False
    
    # ä¿å­˜é…ç½®
    save_only_recent_train: bool = False
    """æ˜¯å¦åªä¿å­˜æœ€è¿‘çš„è®­ç»ƒæ£€æŸ¥ç‚¹"""
    
    # Batch size for training
    batch_size: int = 1
    steps_scaler: float = 1.0
    
    ########### Gaussian Initialization ###############
    init_type: str = "sfm"  # "sfm" or "random"
    init_num_pts: int = 100_000
    init_extent: float = 3.0
    init_opa: float = 0.1
    init_scale: float = 1.0
    global_scale: float = 1.0
    
    ########### Spherical Harmonics ###############
    sh_degree: int = 3
    sh_degree_interval: int = 1000
    
    ########### Loss ###############
    loss_rgb_lambda: float = 0.8  # L1 loss weight
    loss_ssim_lambda: float = 0.2  # SSIM loss weight
    ssim_lambda: float = 0.2  # for compatibility
    
    ########### Rendering ###############
    near_plane: float = 0.01
    far_plane: float = 1e10
    packed: bool = False
    sparse_grad: bool = False
    antialiased: bool = False
    
    ########### Strategy ###############
    strategy: Union[DefaultStrategy, MCMCStrategy] = field(default_factory=DefaultStrategy)

    ########### Background ###############
    random_bkgd: bool = True

    ########### Motion Deblur (BAD-Gaussians) ###############
    camera_optimizer: BadCameraOptimizerConfig = field(
        default_factory=lambda: BadCameraOptimizerConfig(
            mode="linear",
            num_virtual_views=10,  # æ¢å¤åŸå§‹æœ€ä½³é…ç½®
        )
    )

    ########### DiFix3D Integration ###############
    enable_difix3d: bool = True
    """æ˜¯å¦å¯ç”¨DiFix3Då¤„ç†"""
    
    difix3d_model_name: str = "nvidia/difix_ref"
    """DiFix3Dæ¨¡å‹åç§°æˆ–è·¯å¾„"""
    
    difix3d_prompt: str = "remove degradation"
    """DiFix3Då¤„ç†æç¤ºè¯"""
    
    difix3d_blend_ratio: float = 1.0
    """DiFix3Då¢å¼ºå›¾åƒä¸åŸå›¾çš„æ··åˆæ¯”ä¾‹"""
    
    difix3d_num_inference_steps: int = 1
    """DiFix3Dæ¨ç†æ­¥æ•°"""
    
    difix3d_guidance_scale: float = 0.0
    """DiFix3Då¼•å¯¼å°ºåº¦"""

    difix3d_use_ref_image: bool = True
    """æ˜¯å¦ä½¿ç”¨å‚è€ƒå›¾åƒè¿›è¡ŒDiFix3Då¤„ç†"""

    difix3d_augment_training_set: bool = True
    """æ˜¯å¦å°†DiFix3Då¢å¼ºçš„è™šæ‹Ÿè§†è§’æ·»åŠ åˆ°è®­ç»ƒé›†"""
    
    difix3d_max_augmented_samples: int = 100
    """è®­ç»ƒé›†ä¸­æœ€å¤šä¿å­˜çš„å¢å¼ºæ ·æœ¬æ•°é‡"""
    
    difix3d_save_comparisons: bool = True
    """æ˜¯å¦ä¿å­˜DiFix3Då¤„ç†å‰åçš„å¯¹æ¯”å›¾åƒ"""
    
    ########### æ··åˆé‡‡æ ·ç­–ç•¥é…ç½®ï¼ˆç»Ÿä¸€DiFix3Då’Œæ’å¸§å‚æ•°ï¼‰###############
    # ç»Ÿä¸€çš„è™šæ‹Ÿè§†è§’è®­ç»ƒé…ç½®
    virtual_view_start_step: int = 25000
    """å¼€å§‹ä½¿ç”¨è™šæ‹Ÿè§†è§’è®­ç»ƒçš„æ­¥æ•°ï¼ˆæ—©æœŸå¯åŠ¨ä»¥è·å¾—æ›´å¥½æ•ˆæœï¼‰"""
    
    virtual_view_interval: int = 250
    """è™šæ‹Ÿè§†è§’ç”Ÿæˆé—´éš”ï¼ˆæ­¥æ•°ï¼‰"""
    
    virtual_view_poses_per_step: int = 2
    """æ¯æ­¥ç”Ÿæˆçš„è™šæ‹Ÿè§†è§’poseæ•°é‡"""
    
    virtual_view_loss_weight: float = 0.1
    """è™šæ‹Ÿè§†è§’Lossçš„æƒé‡ï¼Œç”¨äºå¹³è¡¡åŸå§‹Losså’Œè™šæ‹Ÿè§†è§’Loss"""

    # æ’å¸§è´¨é‡é˜ˆå€¼ï¼ˆPSNRå·®å€¼èŒƒå›´åˆ¤æ–­ï¼‰
    interp_quality_psnr_min: float = 4.5
    """æ’å€¼å¸§è´¨é‡è¯„åˆ†ï¼ˆPSNRå·®å€¼ï¼‰ä¸‹é™ï¼Œè´¨é‡éœ€å¤§äºè¯¥å€¼"""
    interp_quality_psnr_max: float = 14.5
    """æ’å€¼å¸§è´¨é‡è¯„åˆ†ï¼ˆPSNRå·®å€¼ï¼‰ä¸Šé™ï¼Œè´¨é‡éœ€å°äºè¯¥å€¼"""
    
    ########### Camera Opt ###############
    pose_opt: bool = True
    pose_opt_lr: float = 5e-3
    pose_opt_reg: float = 1e-6
    pose_opt_lr_decay: float = 1e-2
    pose_noise: float = 1e-2
    pose_gradient_accumulation_steps: int = 10

    ########### Appearance Opt ###############
    app_opt: bool = False
    app_embed_dim: int = 32
    app_opt_lr: float = 1e-3
    app_opt_reg: float = 0.0

    ########### Bilateral Grid ###############
    use_bilateral_grid: bool = False
    bilateral_grid_shape: List[int] = field(default_factory=lambda: [16, 16, 8])
    
    ########### Novel View Eval ###############
    nvs_eval_enable_during_training: bool = True
    nvs_steps: int = 200
    nvs_steps_final: int = 1000
    nvs_pose_lr: float = 1e-3
    nvs_pose_reg: float = 0.0
    nvs_pose_lr_decay: float = 1e-2
    
    ########### Deblurring Eval ###############
    deblur_eval_enable_during_training: bool = False
    deblur_eval_enable_pose_opt: bool = False
    
    ########### Regularizations ###############
    enable_phys_scale_reg: bool = False
    max_gauss_ratio: float = 10.0
    enable_mcmc_opacity_reg: bool = False
    enable_mcmc_scale_reg: bool = True
    opacity_reg: float = 0.01
    scale_reg: float = 0.01
    
    ########### Depth Smooth Loss ###############
    enable_depth_smooth_loss: bool = True
    """æ˜¯å¦å¯ç”¨æ·±åº¦å›¾å¹³æ»‘æŸå¤±"""
    depth_smooth_lambda: float = 0.1
    """æ·±åº¦å¹³æ»‘æŸå¤±çš„æƒé‡"""
    
    ########### DiFix Enhancement Loss ###############
    enable_difix_enhancement_loss: bool = True
    """æ˜¯å¦å¯ç”¨DiFixå¢å¼ºå‰åçš„æŸå¤±è®¡ç®—"""
    difix_enhancement_loss_weight: float = 0.05
    """DiFixå¢å¼ºå‰åæŸå¤±çš„æƒé‡"""
    difix_enhancement_l1_weight: float = 0.8
    """DiFixå¢å¼ºæŸå¤±ä¸­L1æŸå¤±çš„æƒé‡"""
    difix_enhancement_perceptual_weight: float = 0.2
    """DiFixå¢å¼ºæŸå¤±ä¸­æ„ŸçŸ¥æŸå¤±çš„æƒé‡"""
    
    # Avoid multiple initialization
    bad_gaussians_post_init_complete: bool = False

    def __post_init__(self):
        if not self.bad_gaussians_post_init_complete:
            self.bad_gaussians_post_init_complete = True
            timestr = time.strftime("%Y%m%d-%H%M%S")
            self.result_dir = Path(self.result_dir) / timestr
            if isinstance(self.strategy, DefaultStrategy):
                self.strategy.grow_grad2d = self.strategy.grow_grad2d / self.camera_optimizer.num_virtual_views
                self.strategy.reset_every = 999999999


def depth_smooth_loss_4neighbor(depth_map: torch.Tensor) -> torch.Tensor:
    """
    è®¡ç®—æ·±åº¦å›¾çš„4é‚»åŸŸå·®åˆ†L2å¹³æ»‘æŸå¤±
    
    Args:
        depth_map: æ·±åº¦å›¾å¼ é‡ [B, H, W] æˆ– [B, H, W, 1]
        
    Returns:
        å¹³æ»‘æŸå¤±å€¼
    """
    # ç¡®ä¿è¾“å…¥æ ¼å¼ä¸º [B, H, W]
    if depth_map.dim() == 4:
        depth_map = depth_map.squeeze(-1)  # [B, H, W, 1] -> [B, H, W]
    
    if depth_map.dim() != 3:
        raise ValueError(f"æ·±åº¦å›¾ç»´åº¦åº”ä¸º3 [B, H, W]ï¼Œå®é™…ä¸º: {depth_map.shape}")
    
    batch_size, height, width = depth_map.shape
    
    # è®¡ç®—æ°´å¹³å’Œå‚ç›´æ–¹å‘çš„å·®åˆ†
    # æ°´å¹³å·®åˆ†ï¼šdepth[i, j] - depth[i, j-1] (é™¤äº†å·¦è¾¹ç•Œ)
    diff_h = depth_map[:, :, 1:] - depth_map[:, :, :-1]  # [B, H, W-1]
    
    # å‚ç›´å·®åˆ†ï¼šdepth[i, j] - depth[i-1, j] (é™¤äº†ä¸Šè¾¹ç•Œ)
    diff_v = depth_map[:, 1:, :] - depth_map[:, :-1, :]  # [B, H-1, W]
    
    # è®¡ç®—L2æŸå¤±
    smooth_loss_h = torch.mean(diff_h ** 2)  # æ°´å¹³æ–¹å‘å¹³æ»‘æŸå¤±
    smooth_loss_v = torch.mean(diff_v ** 2)  # å‚ç›´æ–¹å‘å¹³æ»‘æŸå¤±
    
    # æ€»å¹³æ»‘æŸå¤±
    total_smooth_loss = smooth_loss_h + smooth_loss_v
    
    return total_smooth_loss


class DiFix3DProcessor:
    """DiFix3Då›¾åƒå¤„ç†å™¨ - ç¨³å®šç‰ˆæœ¬ï¼Œä¿æŒåŸå§‹æ•°æ®è´¨é‡"""
    
    def __init__(self, model_name: str = "nvidia/difix_ref", device: str = "cuda", ref_image_dir: str = None):
        self.device = device
        self.model_name = model_name
        self.pipeline = None
        self.enabled = DIFIX3D_AVAILABLE
        self.ref_image_dir = ref_image_dir  # æ·»åŠ ref_image_dirå±æ€§
        
        # æ¸è¿›å¼æ’å€¼ç›¸å…³å±æ€§
        self.is_initialized = False
        self.quality_scorer = None
        self.available_interpolation_views = []
        self.training_psnr_mean = None
        self.training_psnr_variance = None
        
        # DiFix3Då¯¹æ¯”å›¾åƒä¿å­˜ç›®å½•
        self.difix3d_comparison_dir = None
        
        # ç”¨äºå­˜å‚¨è™šæ‹Ÿè§†è§’è´¨é‡è¯„åˆ†æ•°æ®
        self.virtual_view_scores = []
        # ç”¨äºå­˜å‚¨åŸºç¡€æ‰“åˆ†æ•°æ®ï¼ˆè®­ç»ƒè§†è§’PSNRåŸºå‡†ï¼‰
        self.baseline_scores = {}
        
        if self.enabled:
            self._initialize_pipeline()
    
    def _initialize_pipeline(self):
        """åˆå§‹åŒ–DiFix3Dç®¡é“"""
        try:            
            print(f"ğŸ”„ åŠ è½½DiFix3Dæ¨¡å‹: {self.model_name}")
            self.pipeline = DifixPipeline.from_pretrained(
                self.model_name, 
                trust_remote_code=True
            )
            self.pipeline.to(self.device)
            print(f"âœ… DiFix3Dæ¨¡å‹åŠ è½½å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ DiFix3Dæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.enabled = False
    
    def _ensure_tensor_format(self, image_tensor: torch.Tensor) -> tuple[torch.Tensor, tuple[int, int]]:
        """
        ç¡®ä¿è¾“å…¥å¼ é‡æ ¼å¼æ­£ç¡®å¹¶è¿”å›æ ‡å‡†åŒ–çš„å¼ é‡å’ŒåŸå§‹å°ºå¯¸
        
        Args:
            image_tensor: è¾“å…¥å›¾åƒå¼ é‡
            
        Returns:
            tuple: (æ ‡å‡†åŒ–å¼ é‡, (åŸå§‹é«˜åº¦, åŸå§‹å®½åº¦))
        """
        # è®°å½•åŸå§‹å°ºå¯¸
        if image_tensor.dim() == 4:  # [1, H, W, 3]
            original_height, original_width = image_tensor.shape[1:3]
            tensor = image_tensor.squeeze(0)  # [H, W, 3]
        elif image_tensor.dim() == 3:  # [H, W, 3]
            original_height, original_width = image_tensor.shape[:2]
            tensor = image_tensor
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¼ é‡ç»´åº¦: {image_tensor.shape}")
        
        # æ£€æŸ¥é€šé“æ•°
        if tensor.shape[-1] != 3:
            raise ValueError(f"ä¸æ”¯æŒçš„é€šé“æ•°: {tensor.shape[-1]}, æœŸæœ›3")
        
        return tensor, (original_height, original_width)
    
    def process_image(
        self, 
        image_tensor: torch.Tensor, 
        prompt: str = "remove degradation",
        num_inference_steps: int = 1,
        timesteps: List[int] = [199],
        guidance_scale: float = 0.0,
        ref_image: Optional[torch.Tensor] = None,
        save_comparison: bool = False,
        save_path: Optional[str] = None
    ) -> torch.Tensor:
        """
        å¤„ç†å›¾åƒå¼ é‡ - ç¨³å®šç‰ˆæœ¬ï¼Œä¿æŒåŸå§‹å°ºå¯¸å’Œè´¨é‡
        
        Args:
            image_tensor: è¾“å…¥å›¾åƒå¼ é‡ [H, W, 3] æˆ– [1, H, W, 3] èŒƒå›´[0,1]
            prompt: å¤„ç†æç¤ºè¯
            num_inference_steps: æ¨ç†æ­¥æ•°
            timesteps: æ—¶é—´æ­¥åˆ—è¡¨
            guidance_scale: å¼•å¯¼å°ºåº¦
            ref_image: å¯é€‰çš„å‚è€ƒå›¾åƒå¼ é‡ [H, W, 3] æˆ– [1, H, W, 3] èŒƒå›´[0,1]
            save_comparison: æ˜¯å¦ä¿å­˜å¤„ç†å‰åå¯¹æ¯”å›¾åƒ
            save_path: ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            å¤„ç†åçš„å›¾åƒå¼ é‡ï¼Œä¿æŒåŸå§‹å°ºå¯¸å’Œæ ¼å¼
        """
        if not self.enabled or self.pipeline is None:
            return image_tensor
        
        try:
            with torch.no_grad():
                # æ ‡å‡†åŒ–è¾“å…¥å¼ é‡å¹¶è·å–åŸå§‹å°ºå¯¸
                input_tensor, original_size = self._ensure_tensor_format(image_tensor)
                
                
                # ç¡®ä¿å€¼èŒƒå›´åœ¨[0,1]
                if input_tensor.max() > 1.0 or input_tensor.min() < 0.0:
                    input_tensor = torch.clamp(input_tensor, 0.0, 1.0)
                    print(f"   âš ï¸ æ•°å€¼èŒƒå›´å·²è°ƒæ•´åˆ°[0,1]")
                
                # è½¬æ¢ä¸ºPILå›¾åƒ
                image_np = (input_tensor.cpu().numpy() * 255).astype(np.uint8)
                input_image = Image.fromarray(image_np)
                print(f"   PILå›¾åƒå°ºå¯¸: {input_image.size}")  # (width, height)
                
                # å¤„ç†å‚è€ƒå›¾åƒï¼ˆå¦‚æœæä¾›ï¼‰
                ref_image_pil = None
                if ref_image is not None:
                    ref_tensor, _ = self._ensure_tensor_format(ref_image)
                    # ç¡®ä¿å€¼èŒƒå›´åœ¨[0,1]
                    if ref_tensor.max() > 1.0 or ref_tensor.min() < 0.0:
                        ref_tensor = torch.clamp(ref_tensor, 0.0, 1.0)
                    ref_np = (ref_tensor.cpu().numpy() * 255).astype(np.uint8)
                    ref_image_pil = Image.fromarray(ref_np)
                    print(f"   å‚è€ƒå›¾åƒå°ºå¯¸: {ref_image_pil.size}")
                
                # DiFix3Då¤„ç†
                print(f"ğŸ”„ åº”ç”¨DiFix3Då¢å¼º: {prompt}")
                
                # ä¿®å¤ï¼šç¡®ä¿è¾“å…¥å›¾åƒå’Œå‚è€ƒå›¾åƒå°ºå¯¸å®Œå…¨åŒ¹é…
                if ref_image_pil is not None:
                    # ä½¿ç”¨å‚è€ƒå›¾åƒçš„å¤„ç†æ–¹å¼
                    print(f"   ğŸ“· ä½¿ç”¨å‚è€ƒå›¾åƒè¿›è¡ŒDiFix3Då¤„ç†")
                    
                    # ç¡®ä¿è¾“å…¥å›¾åƒå’Œå‚è€ƒå›¾åƒå°ºå¯¸å®Œå…¨åŒ¹é…
                    if input_image.size != ref_image_pil.size:
                        print(f"   ğŸ”§ è°ƒæ•´å‚è€ƒå›¾åƒå°ºå¯¸ä»¥åŒ¹é…è¾“å…¥å›¾åƒ: {input_image.size} -> {ref_image_pil.size}")
                        ref_image_pil = ref_image_pil.resize(input_image.size, Image.Resampling.LANCZOS)
                    
                    # æ£€æŸ¥å›¾åƒå°ºå¯¸æ˜¯å¦åˆç†
                    width, height = input_image.size
                    if width * height > 1000000:  # å¦‚æœå›¾åƒå¤ªå¤§ï¼Œå¯èƒ½å¯¼è‡´å†…å­˜é—®é¢˜
                        print(f"   âš ï¸ å›¾åƒå°ºå¯¸è¾ƒå¤§ ({width}x{height})ï¼Œå¯èƒ½å¯¼è‡´å†…å­˜é—®é¢˜")
                        # å¯ä»¥é€‰æ‹©ç¼©å°å›¾åƒï¼Œä½†è¿™é‡Œå…ˆå°è¯•ç›´æ¥å¤„ç†
                    
                    # ç›´æ¥ä½¿ç”¨å•å¼ å›¾åƒï¼Œä¸å¤åˆ¶ä¸ºbatch
                    try:
                        output_image = self.pipeline(
                            prompt,
                            image=input_image,
                            ref_image=ref_image_pil,
                            num_inference_steps=num_inference_steps,
                            timesteps=timesteps,
                            guidance_scale=guidance_scale
                        ).images[0]
                    except Exception as e:
                        print(f"   âš ï¸ å•å¼ å›¾åƒå¤„ç†å¤±è´¥ï¼Œå°è¯•batchå¤„ç†: {e}")
                        # å¦‚æœå•å¼ å›¾åƒå¤±è´¥ï¼Œå°è¯•batchå¤„ç†
                        input_images = [input_image, input_image]
                        ref_images = [ref_image_pil, ref_image_pil]
                        
                        output_images = self.pipeline(
                            prompt,
                            image=input_images,
                            ref_image=ref_images,
                            num_inference_steps=num_inference_steps,
                            timesteps=timesteps,
                            guidance_scale=guidance_scale
                        ).images
                        output_image = output_images[0]
                else:
                    # ä¸ä½¿ç”¨å‚è€ƒå›¾åƒçš„å¤„ç†æ–¹å¼
                    print(f"   ğŸš« ä¸ä½¿ç”¨å‚è€ƒå›¾åƒï¼Œç›´æ¥è¿›è¡ŒDiFix3Då¤„ç†")
                    
                    # ç›´æ¥ä½¿ç”¨å•å¼ å›¾åƒ
                    try:
                        output_image = self.pipeline(
                            prompt,
                            image=input_image,
                            num_inference_steps=num_inference_steps,
                            timesteps=timesteps,
                            guidance_scale=guidance_scale
                        ).images[0]
                    except Exception as e:
                        print(f"   âš ï¸ å•å¼ å›¾åƒå¤„ç†å¤±è´¥ï¼Œå°è¯•batchå¤„ç†: {e}")
                        # å¦‚æœå•å¼ å›¾åƒå¤±è´¥ï¼Œå°è¯•batchå¤„ç†
                        input_images = [input_image, input_image]
                        
                        output_images = self.pipeline(
                            prompt,
                            image=input_images,
                            num_inference_steps=num_inference_steps,
                            timesteps=timesteps,
                            guidance_scale=guidance_scale
                        ).images
                        output_image = output_images[0]
                
                print(f"   DiFix3Dè¾“å‡ºPILå°ºå¯¸: {output_image.size}")  # (width, height)
                
                # è½¬å›å¼ é‡
                output_np = np.array(output_image).astype(np.float32) / 255.0
                output_tensor = torch.from_numpy(output_np).to(image_tensor.device)
                
                print(f"   è½¬æ¢åå¼ é‡å½¢çŠ¶: {output_tensor.shape}")
                
                # å¦‚æœåŸå§‹è¾“å…¥æœ‰batchç»´åº¦ï¼Œæ·»åŠ å›æ¥
                if image_tensor.dim() == 4:
                    output_tensor = output_tensor.unsqueeze(0)  # [1, H, W, 3]
                
                print(f"âœ… DiFix3Då¤„ç†å®Œæˆ:")
                print(f"   æœ€ç»ˆè¾“å‡ºå½¢çŠ¶: {output_tensor.shape}")
                print(f"   å°ºå¯¸å˜åŒ–: {original_size} -> {output_tensor.shape[1:3] if output_tensor.dim() == 4 else output_tensor.shape[:2]}")
                
                # æ£€æŸ¥å°ºå¯¸æ˜¯å¦å‘ç”Ÿå˜åŒ–
                final_size = output_tensor.shape[1:3] if output_tensor.dim() == 4 else output_tensor.shape[:2]
                if final_size != original_size:
                    print(f"   âš ï¸ å°ºå¯¸å‘ç”Ÿå˜åŒ–: {original_size} -> {final_size}")
                
                # ä¿å­˜å¤„ç†å‰åå¯¹æ¯”å›¾åƒ
                return output_tensor
                
        except Exception as e:
            print(f"âš ï¸ DiFix3Då¤„ç†å¤±è´¥: {e}")
            print(f"   è¾“å…¥å¼ é‡å½¢çŠ¶: {image_tensor.shape}, æ•°æ®ç±»å‹: {image_tensor.dtype}")
            print(f"   é”™è¯¯è¯¦æƒ…: {str(e)}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯einopsç›¸å…³çš„é”™è¯¯
            if "einops" in str(e).lower() or "rearrange" in str(e).lower():
                print(f"   ğŸ”§ æ£€æµ‹åˆ°einopså¼ é‡é‡æ’é”™è¯¯ï¼Œå°è¯•ä½¿ç”¨å•å¼ å›¾åƒå¤„ç†...")
                try:
                    # å°è¯•ä½¿ç”¨å•å¼ å›¾åƒï¼Œä½†æ·»åŠ batchç»´åº¦
                    single_output = self.pipeline(
                        prompt,
                        image=input_image,
                        num_inference_steps=num_inference_steps,
                        timesteps=timesteps,
                        guidance_scale=guidance_scale
                    ).images[0]
                    
                    # è½¬æ¢å›å¼ é‡
                    output_np = np.array(single_output).astype(np.float32) / 255.0
                    output_tensor = torch.from_numpy(output_np).to(image_tensor.device)
                    
                    if image_tensor.dim() == 4:
                        output_tensor = output_tensor.unsqueeze(0)
                    
                    print(f"   âœ… å•å¼ å›¾åƒå¤„ç†æˆåŠŸ")
                    return output_tensor
                    
                except Exception as e2:
                    print(f"   âŒ å•å¼ å›¾åƒå¤„ç†ä¹Ÿå¤±è´¥: {e2}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¼ é‡ç»´åº¦ä¸åŒ¹é…é”™è¯¯
            elif "size of tensor" in str(e).lower() and "must match" in str(e).lower():
                print(f"   ğŸ”§ æ£€æµ‹åˆ°å¼ é‡ç»´åº¦ä¸åŒ¹é…é”™è¯¯ï¼Œå°è¯•è·³è¿‡DiFix3Då¤„ç†...")
                print(f"   âš ï¸ DiFix3Då¤„ç†å¤±è´¥ï¼Œè¿”å›åŸå§‹å›¾åƒ")
                return image_tensor
            
            import traceback
            traceback.print_exc()
            # è¿”å›åŸå§‹è¾“å…¥ï¼Œç¡®ä¿è®­ç»ƒç»§ç»­è¿›è¡Œ
            return image_tensor
    
    def load_ref_image(self, train_idx: int, trainset) -> Optional[torch.Tensor]:
        """
        æ ¹æ®è®­ç»ƒé›†ç´¢å¼•ä»ref_imageç›®å½•åŠ è½½å‚è€ƒå›¾åƒ
        
        Args:
            train_idx: è®­ç»ƒé›†å†…çš„ç´¢å¼•ï¼ˆ0, 1, 2ç­‰ï¼‰
            trainset: è®­ç»ƒæ•°æ®é›†ï¼Œç”¨äºè·å–å¯¹åº”çš„COLMAPç´¢å¼•
            
        Returns:
            åŠ è½½çš„å‚è€ƒå›¾åƒå¼ é‡ [H, W, 3]ï¼Œå¦‚æœåŠ è½½å¤±è´¥åˆ™è¿”å›None
        """
        try:
            # æ£€æŸ¥ref_image_diræ˜¯å¦è®¾ç½®
            if self.ref_image_dir is None:
                print(f"âš ï¸ ref_image_diræœªè®¾ç½®ï¼Œæ— æ³•åŠ è½½å‚è€ƒå›¾åƒ")
                return None
            
            # ä»è®­ç»ƒé›†è·å–å¯¹åº”çš„COLMAPç´¢å¼•
            try:
                train_data = trainset[train_idx]
                colmap_idx = train_data["colmap_image_id"]
                if isinstance(colmap_idx, torch.Tensor):
                    colmap_idx = colmap_idx.item()
                
                print(f"ğŸ” è®­ç»ƒé›†ç´¢å¼• {train_idx} -> COLMAPç´¢å¼• {colmap_idx}")
                
            except Exception as e:
                print(f"âŒ æ— æ³•è·å–COLMAPç´¢å¼• (train_idx={train_idx}): {e}")
                return None
            
            # æ„å»ºå‚è€ƒå›¾åƒè·¯å¾„ï¼ˆä½¿ç”¨COLMAPç´¢å¼•ï¼‰
            ref_image_path = f"{self.ref_image_dir}/{colmap_idx:03d}.png"
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(ref_image_path):
                print(f"âš ï¸ å‚è€ƒå›¾åƒä¸å­˜åœ¨: {ref_image_path}")
                return None
            
            # åŠ è½½å›¾åƒ
            from PIL import Image
            import numpy as np
            
            ref_image_pil = Image.open(ref_image_path).convert('RGB')
            ref_image_np = np.array(ref_image_pil) / 255.0  # å½’ä¸€åŒ–åˆ°[0,1]
            ref_image_tensor = torch.from_numpy(ref_image_np).float().to(self.device)
            
            print(f"âœ… æˆåŠŸåŠ è½½å‚è€ƒå›¾åƒ: {ref_image_path}, å°ºå¯¸: {ref_image_tensor.shape}")
            return ref_image_tensor
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å‚è€ƒå›¾åƒå¤±è´¥ (train_idx={train_idx}): {e}")
            return None
    
    def process_virtual_views_batch(
        self, 
        trainset, 
        camera_optimizer, 
        rasterize_splats_fn,
        cfg,
        step: int,
        ref_image: Optional[torch.Tensor] = None,
        save_comparisons: bool = True,
        comparison_dir: Optional[str] = None
    ) -> List[dict]:
        """
        å¤„ç†è™šæ‹Ÿè§†è§’æ‰¹æ¬¡ - é€‰æ‹©ä¸¤ä¸ªè§†è§’ï¼Œç”Ÿæˆæ’å€¼å¸§ï¼Œè´¨é‡è¯„ä¼°ï¼ŒDiFix3Då¢å¼º
        
        Args:
            trainset: è®­ç»ƒæ•°æ®é›†
            camera_optimizer: BAD-Gaussiansç›¸æœºä¼˜åŒ–å™¨
            rasterize_splats_fn: 3DGSæ¸²æŸ“å‡½æ•°
            cfg: é…ç½®å¯¹è±¡
            step: å½“å‰è®­ç»ƒæ­¥æ•°
            ref_image: å¯é€‰çš„å‚è€ƒå›¾åƒï¼ˆæœªä½¿ç”¨ï¼Œä¿æŒæ¥å£å…¼å®¹æ€§ï¼‰
            save_comparisons: æ˜¯å¦ä¿å­˜DiFix3Då¤„ç†å‰åå¯¹æ¯”å›¾åƒ
            comparison_dir: å¯¹æ¯”å›¾åƒä¿å­˜ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤ç›®å½•
            
        Returns:
            å¢å¼ºæ ·æœ¬åˆ—è¡¨ List[dict]
        """
        # è®¾ç½®å¯¹æ¯”å›¾åƒä¿å­˜ç›®å½•
        if comparison_dir is not None:
            self.difix3d_comparison_dir = comparison_dir
        
        if not self.enabled or self.pipeline is None:
            print("âš ï¸ DiFix3Dæœªå¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ‰¹é‡å¤„ç†")
            return []
        
        if not hasattr(trainset, '__len__') or len(trainset) == 0:
            print("âš ï¸ è®­ç»ƒé›†ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæ‰¹é‡å¤„ç†")
            return []
        
        # ç¡®ä¿æ’å€¼æ± å·²åˆå§‹åŒ–
        if not self.is_initialized:
            self.initialize_interpolation_pool(trainset, rasterize_splats_fn, cfg)
            if not self.is_initialized:
                print("âŒ æ’å€¼æ± åˆå§‹åŒ–å¤±è´¥")
                return []
        
        print(f"ğŸ¯ æ­¥æ•° {step}: å¼€å§‹å¤„ç†è™šæ‹Ÿè§†è§’æ‰¹æ¬¡")
        
        enhanced_samples = []
        quality_threshold = 0  # k <= 0 è¡¨ç¤ºè´¨é‡å¯æ¥å—
        
        try:
            # 1. é€‰æ‹©æ’å€¼ç­–ç•¥
            if len(self.available_interpolation_views) < 1:
                print(f"âŒ æ’å€¼æ± è§†è§’ä¸è¶³ ({len(self.available_interpolation_views)} < 1)ï¼Œæ— æ³•è¿›è¡Œæ’å€¼")
                return []
            
            # éšæœºé€‰æ‹©ä¸¤ä¸ªä¸åŒçš„è®­ç»ƒè§†è§’ä½œä¸ºå‰å‘æ’å€¼çš„åŸºç¡€
            train_indices = torch.randperm(len(trainset))[:2]
            train_view1 = trainset[train_indices[0]]
            train_view2 = trainset[train_indices[1]]
            
            # åå‘æ’å€¼ï¼šé€‰æ‹©ä¸¤ä¸ªä¸åŒçš„è™šæ‹Ÿè§†è§’
            print(f"   ğŸ” è™šæ‹Ÿè§†è§’æ± çŠ¶æ€: {len(self.available_interpolation_views)} ä¸ªå¯ç”¨è§†è§’")
            if len(self.available_interpolation_views) >= 2:
                virtual_indices = torch.randperm(len(self.available_interpolation_views))[:2]
                virtual_view1 = self.available_interpolation_views[virtual_indices[0]]
                virtual_view2 = self.available_interpolation_views[virtual_indices[1]]
                use_backward_interpolation = True
                print(f"   âœ… åå‘æ’å€¼å¯ç”¨: é€‰æ‹©è™šæ‹Ÿè§†è§’ {virtual_indices[0]} å’Œ {virtual_indices[1]}")
            else:
                # å¦‚æœè™šæ‹Ÿè§†è§’ä¸è¶³ï¼Œåªä½¿ç”¨å‰å‘æ’å€¼
                use_backward_interpolation = False
                virtual_view1 = None
                virtual_view2 = None
            print(f"   å‰å‘æ’å€¼åŸºç¡€: è®­ç»ƒè§†è§’ {train_indices[0]} å’Œ {train_indices[1]}")
            if use_backward_interpolation:
                print(f"   åå‘æ’å€¼åŸºç¡€: è™šæ‹Ÿè§†è§’ {virtual_view1['source']} å’Œ {virtual_view2['source']}")
            else:
                print(f"   åå‘æ’å€¼: è·³è¿‡ï¼ˆè™šæ‹Ÿè§†è§’ä¸è¶³ï¼Œéœ€è¦è‡³å°‘2ä¸ªï¼‰")
            
            # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥åŸºç¡€è§†è§’
            print(f"   ğŸ” åŸºç¡€è§†è§’è°ƒè¯•:")
            train_pos1 = train_view1['camtoworld'][:3, 3].to(self.device)
            train_pos2 = train_view2['camtoworld'][:3, 3].to(self.device)
            if use_backward_interpolation:
                virtual_pos1 = virtual_view1['pose'][:3, 3].to(self.device)
                virtual_pos2 = virtual_view2['pose'][:3, 3].to(self.device)
                
            
            # 2. ç”Ÿæˆå‰å‘å’Œåå‘æ’å€¼å¸§
            forward_alpha = 0.5  # å‰å‘æ’å€¼ï¼šåœ¨è®­ç»ƒè§†è§’ä¹‹é—´
            backward_alpha = 1.5  # åå‘æ’å€¼ï¼šåœ¨è™šæ‹Ÿè§†è§’ä¹‹å¤–ï¼ˆå‘å¤–æ¢ç´¢ï¼‰
            
            quality_scores = []
            interpolated_poses = []
            
            # å‰å‘æ’å€¼ï¼šè®­ç»ƒè§†è§’ä¹‹é—´
            print(f"   ğŸ¯ å‰å‘æ’å€¼ï¼šè®­ç»ƒè§†è§’ä¹‹é—´ (Î±={forward_alpha})")
            # ç¡®ä¿è®­ç»ƒè§†è§’æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            train_pose1 = train_view1["camtoworld"].to(self.device)
            train_K1 = train_view1["K"].to(self.device)
            train_pose2 = train_view2["camtoworld"].to(self.device)
            train_K2 = train_view2["K"].to(self.device)
            
            interpolated_pose_forward, _ = se3_interpolate_to_target(
                train_pose1, train_K1, 
                train_pose2, train_K2, 
                t=forward_alpha
            )
            interpolated_poses.append(interpolated_pose_forward)
            
            # åå‘æ’å€¼ï¼šè™šæ‹Ÿè§†è§’ä¹‹å¤–ï¼ˆå‘å¤–æ¢ç´¢ï¼‰
            interpolated_pose_backward = None
            if use_backward_interpolation:
                print(f"   ğŸ¯ åå‘æ’å€¼ï¼šè™šæ‹Ÿè§†è§’ä¹‹å¤–å‘å¤–æ¢ç´¢ (Î±={backward_alpha})")
                # ç¡®ä¿è™šæ‹Ÿè§†è§’æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                virtual_pose1 = virtual_view1["pose"].to(self.device)
                virtual_K1 = virtual_view1["K"].to(self.device)
                virtual_pose2 = virtual_view2["pose"].to(self.device)
                virtual_K2 = virtual_view2["K"].to(self.device)
                
                # ä½¿ç”¨åå‘æ’å€¼å‘å¤–æ¢ç´¢ï¼šä»virtual_pose1å‘virtual_pose2æ–¹å‘å»¶ä¼¸
                # t=1.5 æ„å‘³ç€åœ¨virtual_pose2ä¹‹å¤–0.5å€è·ç¦»çš„ä½ç½®
                interpolated_pose_backward, _ = se3_interpolate_to_target(
                    virtual_pose1, virtual_K1, 
                    virtual_pose2, virtual_K2, 
                    t=backward_alpha
                )
                interpolated_poses.append(interpolated_pose_backward)
                
                # è°ƒè¯•ï¼šæ£€æŸ¥åå‘æ’å€¼åçš„ç›¸æœºä½ç½®
                interp_pos_backward = interpolated_pose_backward[:3, 3]
                virtual_pos1 = virtual_pose1[:3, 3]
                virtual_pos2 = virtual_pose2[:3, 3]
            else:
                print(f"   ğŸš« åå‘æ’å€¼ï¼šè·³è¿‡ï¼ˆè™šæ‹Ÿè§†è§’ä¸è¶³ï¼‰")
            
            # 3. ç”Ÿæˆæ’å€¼å¸§
            print(f"   âœ… å¼€å§‹ç”Ÿæˆ {cfg.virtual_view_poses_per_step} ä¸ªæ’å€¼å¸§")
            
            for i in range(cfg.virtual_view_poses_per_step):
                # äº¤æ›¿ç”Ÿæˆå‰å‘å’Œåå‘æ’å€¼
                if i == 0:
                    # å‰å‘æ’å€¼ï¼šè®­ç»ƒè§†è§’ä¹‹é—´
                    interpolated_pose = interpolated_pose_forward
                    direction = "å‰å‘"
                    alpha = forward_alpha
                elif i == 1 and use_backward_interpolation:
                    # åå‘æ’å€¼ï¼šè™šæ‹Ÿè§†è§’ä¹‹å¤–å‘å¤–æ¢ç´¢
                    interpolated_pose = interpolated_pose_backward
                    direction = "åå‘"
                    alpha = backward_alpha
                elif i == 1 and not use_backward_interpolation:
                    # å¦‚æœåå‘æ’å€¼ä¸å¯ç”¨ï¼Œè·³è¿‡
                    continue
                else:
                    # å¦‚æœè¶…è¿‡2ä¸ªï¼Œéšæœºé€‰æ‹©æ’å€¼ç­–ç•¥
                    if torch.rand(1).item() < 0.5:
                        # è®­ç»ƒè§†è§’ä¹‹é—´
                        alpha = torch.rand(1).item() * 0.8 + 0.1
                        interpolated_pose, _ = se3_interpolate_to_target(
                            train_pose1, train_K1, 
                            train_pose2, train_K2, 
                            t=alpha
                        )
                        direction = "å‰å‘éšæœº"
                    elif use_backward_interpolation:
                        # è™šæ‹Ÿè§†è§’ä¹‹å¤–å‘å¤–æ¢ç´¢ï¼ˆéšæœºé€‰æ‹©æ¢ç´¢æ–¹å‘ï¼‰
                        if torch.rand(1).item() < 0.5:
                            # å‘å‰æ¢ç´¢ï¼št > 1.0
                            alpha = torch.rand(1).item() * 0.5 + 1.0  # [1.0, 1.5]
                            interpolated_pose, _ = se3_interpolate_to_target(
                                virtual_pose1, virtual_K1, 
                                virtual_pose2, virtual_K2, 
                                t=alpha
                            )
                            direction = "åå‘éšæœº-å‘å‰æ¢ç´¢"
                        else:
                            # å‘åæ¢ç´¢ï¼št < 0.0
                            alpha = torch.rand(1).item() * 0.5 - 0.5  # [-0.5, 0.0]
                            interpolated_pose, _ = se3_interpolate_to_target(
                                virtual_pose2, virtual_K2, 
                                virtual_pose1, virtual_K1, 
                                t=alpha
                            )
                            direction = "åå‘éšæœº-å‘åæ¢ç´¢"
                    else:
                        # å¦‚æœåå‘æ’å€¼ä¸å¯ç”¨ï¼Œä½¿ç”¨å‰å‘æ’å€¼
                        alpha = torch.rand(1).item() * 0.8 + 0.1
                        interpolated_pose, _ = se3_interpolate_to_target(
                            train_pose1, train_K1, 
                            train_pose2, train_K2, 
                            t=alpha
                        )
                        direction = "å‰å‘éšæœº"
                
                print(f"   ğŸ¯ ç”Ÿæˆ{direction}æ’å€¼å¸§ (Î±={alpha:.3f})")
                
                # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥æ’å€¼åçš„ç›¸æœºä½ç½®
                interp_pos = interpolated_pose[:3, 3]
                
                # ä½¿ç”¨è®­ç»ƒè§†è§’çš„å†…å‚å’Œå›¾åƒIDï¼Œç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                interp_K = train_view1["K"].unsqueeze(0).to(self.device)  # [1, 3, 3]
                interp_img_id = train_view1["image_id"].unsqueeze(0).to(self.device)
                
                # ç¡®ä¿æ’å€¼poseåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                interpolated_pose = interpolated_pose.to(self.device)  # [4, 4]
                
                # è·å–å›¾åƒå°ºå¯¸ï¼ˆä½¿ç”¨è®­ç»ƒè§†è§’çš„å°ºå¯¸ï¼‰
                
                # æ£€æŸ¥å›¾åƒå½¢çŠ¶æ˜¯å¦æ­£ç¡®
                if len(train_view1["image"].shape) == 4:  # [1, H, W, 3]
                    height, width = train_view1["image"].shape[1:3]  # [H, W]
                    print(f"     4Då›¾åƒå½¢çŠ¶: [1, {height}, {width}, 3]")
                elif len(train_view1["image"].shape) == 3:  # [H, W, 3]
                    height, width = train_view1["image"].shape[:2]  # [H, W]
                    print(f"     3Då›¾åƒå½¢çŠ¶: [{height}, {width}, 3]")
                else:
                    print(f"     âš ï¸ æ„å¤–çš„å›¾åƒå½¢çŠ¶: {train_view1['image'].shape}")
                    # ä½¿ç”¨é»˜è®¤å°ºå¯¸
                    height, width = 400, 600  # å‡è®¾æ˜¯400x600
                    print(f"     ä½¿ç”¨é»˜è®¤å°ºå¯¸: height={height}, width={width}")
                
                # æ ¹æ®Datasetçš„__getitem__æ–¹æ³•ï¼Œimageå½¢çŠ¶åº”è¯¥æ˜¯[H, W, 3]
                # æ‰€ä»¥æˆ‘ä»¬åº”è¯¥ä½¿ç”¨[:2]æ¥è·å–height, width
                if len(train_view1["image"].shape) == 3:
                    height, width = train_view1["image"].shape[:2]  # [H, W]
                    print(f"     âœ… ä½¿ç”¨3Då›¾åƒå½¢çŠ¶: [{height}, {width}, 3]")
                
                print(f"     æœ€ç»ˆæå–çš„height: {height}, width: {width}")
                
                # æ¸²æŸ“æ’å€¼è§†è§’ï¼ˆåŒ…æ‹¬æ·±åº¦ä¿¡æ¯ï¼‰
                renders_interp, depths_interp, _ = rasterize_splats_fn(
                    camtoworlds=interpolated_pose.unsqueeze(0),  # [1, 4, 4]
                    Ks=interp_K,  # [1, 3, 3]
                    width=width,
                    height=height,
                    sh_degree=cfg.sh_degree,
                    near_plane=cfg.near_plane,
                    far_plane=cfg.far_plane,
                    image_ids=interp_img_id,
                    render_mode="RGB+ED" if cfg.enable_depth_smooth_loss else "RGB",
                )
                
                # ç¡®ä¿æ¸²æŸ“ç»“æœåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                renders_interp = renders_interp.to(self.device)
                if depths_interp is not None:
                    depths_interp = depths_interp.to(self.device)
                
                
                # æ‰¾åˆ°æœ€è¿‘çš„è®­ç»ƒè§†è§’ä½œä¸ºå‚è€ƒ
                nearest_train_idx = self._find_nearest_training_view(interpolated_pose, trainset)
                nearest_train_data = trainset[nearest_train_idx]
                
                # æ¸²æŸ“æœ€è¿‘è®­ç»ƒè§†è§’ä½œä¸ºå‚è€ƒå›¾åƒ
                train_pose = nearest_train_data["camtoworld"].unsqueeze(0).to(self.device)
                train_K = nearest_train_data["K"].unsqueeze(0).to(self.device)
                
                # è·å–è®­ç»ƒè§†è§’ID
                if isinstance(nearest_train_data["image_id"], int):
                    train_view_id = nearest_train_data["image_id"]
                else:
                    train_view_id = nearest_train_data["image_id"].item()
                
                # ä½¿ç”¨DiFix3Då¢å¼ºæ’å€¼è§†è§’
                # ä¸ºæ’å€¼è§†è§’é€‰æ‹©å‚è€ƒå›¾åƒ
                ref_image_for_interp = None
                if cfg.difix3d_use_ref_image:
                    # ä»é¢„è®¾ç›®å½•åŠ è½½å‚è€ƒå›¾åƒï¼ŒåŸºäºè®­ç»ƒé›†ç´¢å¼•
                    ref_image_for_interp = self.load_ref_image(nearest_train_idx, trainset)
                    
                    if ref_image_for_interp is not None:
                        print(f"   ğŸ“· æˆåŠŸä»ç›®å½•åŠ è½½å‚è€ƒå›¾åƒ (train_idx={nearest_train_idx})")
                        print(f"   ğŸ” ref_image_for_interpå½¢çŠ¶: {ref_image_for_interp.shape}")
                        print(f"   ğŸ” ref_image_for_interpè®¾å¤‡: {ref_image_for_interp.device}")
                    else:
                        print(f"   âš ï¸ æ— æ³•ä»ç›®å½•åŠ è½½å‚è€ƒå›¾åƒ (train_idx={nearest_train_idx})ï¼Œå°†ä¸ä½¿ç”¨å‚è€ƒå›¾åƒ")
                else:
                    print(f"   ğŸš« ä¸ä½¿ç”¨å‚è€ƒå›¾åƒè¿›è¡ŒDiFix3Då¤„ç†")
                
                
                # ç¡®ä¿æ¸²æŸ“ç»“æœæ ¼å¼æ­£ç¡®ï¼Œå¤„ç†RGB+EDæ¨¡å¼çš„4é€šé“è¾“å‡º
                if renders_interp[0].dim() != 3:
                    print(f"     âš ï¸ æ¸²æŸ“ç»“æœç»´åº¦ä¸æ­£ç¡®ï¼Œè·³è¿‡DiFix3Då¤„ç†")
                    enhanced_interp = renders_interp[0]  # ç›´æ¥ä½¿ç”¨åŸå§‹æ¸²æŸ“ç»“æœ
                elif renders_interp[0].shape[-1] == 4:
                    # RGB+EDæ¨¡å¼ï¼šåªå–å‰3ä¸ªé€šé“ï¼ˆRGBï¼‰ç”¨äºDiFix3Då¤„ç†
                    print(f"     ğŸ”§ RGB+EDæ¨¡å¼ï¼šæå–RGBé€šé“ç”¨äºDiFix3Då¤„ç†")
                    rgb_interp = renders_interp[0][:, :, :3]  # [H, W, 3]
                    enhanced_interp = self.process_image(
                        rgb_interp,  # [H, W, 3]
                        prompt=cfg.difix3d_prompt,
                        num_inference_steps=cfg.difix3d_num_inference_steps,
                        timesteps=[199],
                        guidance_scale=cfg.difix3d_guidance_scale,
                        ref_image=ref_image_for_interp,
                        save_comparison=cfg.difix3d_save_comparisons,  # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿å­˜
                        save_path=f"{self.difix3d_comparison_dir}/step_{step}_view_{i}_rgb_ed"
                    )
                elif renders_interp[0].shape[-1] == 3:
                    # æ ‡å‡†RGBæ¨¡å¼
                    enhanced_interp = self.process_image(
                        renders_interp[0],  # [H, W, 3]
                        prompt=cfg.difix3d_prompt,
                        num_inference_steps=cfg.difix3d_num_inference_steps,
                        timesteps=[199],
                        guidance_scale=cfg.difix3d_guidance_scale,
                        ref_image=ref_image_for_interp,
                        save_comparison=cfg.difix3d_save_comparisons,  # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿å­˜
                        save_path=f"{self.difix3d_comparison_dir}/step_{step}_view_{i}_rgb"
                    )
                else:
                    print(f"     âš ï¸ æ¸²æŸ“ç»“æœé€šé“æ•°ä¸æ­£ç¡®ï¼Œè·³è¿‡DiFix3Då¤„ç†")
                    enhanced_interp = renders_interp[0]  # ç›´æ¥ä½¿ç”¨åŸå§‹æ¸²æŸ“ç»“æœ
                
                # ç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆåœ¨è´¨é‡è¯„åˆ†è®¡ç®—ä¹‹å‰ï¼‰
                interpolated_pose_device = interpolated_pose.to(self.device)
                interp_K_device = interp_K[0].to(self.device)
                interp_img_id_device = interp_img_id[0].to(self.device)
                
                # è®¡ç®—è´¨é‡è¯„åˆ†ï¼ˆç¡®ä¿ä¸¤ä¸ªè¾“å…¥éƒ½æ˜¯RGBæ ¼å¼ï¼‰
                try:
                    # ç¡®ä¿ç”¨äºè´¨é‡è¯„åˆ†çš„åŸå§‹å›¾åƒä¹Ÿæ˜¯RGBæ ¼å¼
                    if renders_interp[0].shape[-1] == 4:
                        original_rgb_for_score = renders_interp[0][:, :, :3]  # [H, W, 3]
                    else:
                        original_rgb_for_score = renders_interp[0]  # [H, W, 3]
                    
                    _, quality_score = self.quality_scorer.score_pseudo_view(
                        original_rgb_for_score, enhanced_interp
                    )
                    print(f"   ğŸ“Š æ’å€¼å¸§è´¨é‡è¯„åˆ†: k={quality_score:.4f}")
                    
                    # ä¿å­˜è™šæ‹Ÿè§†è§’è´¨é‡è¯„åˆ†æ•°æ®
                    score_data = {
                        "step": step,
                        "view_idx": i,
                        "direction": direction,
                        "alpha": alpha,
                        "quality_score": float(quality_score),
                        "nearest_train_idx": nearest_train_idx,
                        "interpolated_pose": interpolated_pose_device.cpu().numpy().tolist(),
                        "timestamp": time.time()
                    }
                    self.virtual_view_scores.append(score_data)
                    print(f"   ğŸ’¾ å·²ä¿å­˜è™šæ‹Ÿè§†è§’è´¨é‡è¯„åˆ†æ•°æ®")
                    
                except Exception as e:
                    print(f"   âš ï¸ è´¨é‡è¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
                    quality_score = 0.0  # é»˜è®¤è´¨é‡è¯„åˆ†
                    
                    # å³ä½¿è¯„åˆ†å¤±è´¥ä¹Ÿä¿å­˜æ•°æ®
                    score_data = {
                        "step": step,
                        "view_idx": i,
                        "direction": direction,
                        "alpha": alpha,
                        "quality_score": 0.0,
                        "nearest_train_idx": nearest_train_idx,
                        "interpolated_pose": interpolated_pose_device.cpu().numpy().tolist(),
                        "timestamp": time.time(),
                        "error": str(e)
                    }
                    self.virtual_view_scores.append(score_data)
                    print(f"   ğŸ’¾ å·²ä¿å­˜è™šæ‹Ÿè§†è§’è´¨é‡è¯„åˆ†æ•°æ®ï¼ˆè¯„åˆ†å¤±è´¥ï¼‰")
                
                # ç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                enhanced_interp_device = enhanced_interp.to(self.device)
                
                # æ·»åŠ åˆ°å¢å¼ºæ ·æœ¬åˆ—è¡¨ï¼ˆç”¨äºåç»­é‡æ–°æ¸²æŸ“å’ŒæŸå¤±è®¡ç®—ï¼‰
                # âœ… ä¸ä¿å­˜original_imageï¼Œè€Œæ˜¯ä¿å­˜æ¸²æŸ“å‚æ•°ï¼Œåœ¨æ¯ä¸ªè®­ç»ƒæ­¥é‡æ–°æ¸²æŸ“
                sample = {
                    "enhanced_image": enhanced_interp_device.detach().clone(),  # [H, W, 3] - DiFixå¢å¼ºåçš„å›¾åƒï¼ˆä½œä¸ºç›‘ç£ä¿¡å·ï¼‰
                    "pose": interpolated_pose_device.detach().clone(),  # [4, 4] - ç”¨äºé‡æ–°æ¸²æŸ“
                    "K": interp_K_device.detach().clone(),  # [3, 3] - ç”¨äºé‡æ–°æ¸²æŸ“
                    "image_id": interp_img_id_device.detach().clone(),  # ç”¨äºé‡æ–°æ¸²æŸ“
                    "width": width,  # å›¾åƒå®½åº¦
                    "height": height,  # å›¾åƒé«˜åº¦
                    "view_idx": i,
                    "interpolated": True,
                    "alpha": alpha,
                    "nearest_train_idx": nearest_train_idx,
                    "quality_score": quality_score,  # æ·»åŠ è´¨é‡è¯„åˆ†
                }
                
                # è°ƒè¯•ï¼šæ£€æŸ¥sampleä¸­æ‰€æœ‰å¼ é‡çš„è®¾å¤‡
                print(f"   ğŸ” æ ·æœ¬{i}å¼ é‡è®¾å¤‡æ£€æŸ¥:")
                for key, value in sample.items():
                    if isinstance(value, torch.Tensor):
                        print(f"     {key}: è®¾å¤‡={value.device}, å½¢çŠ¶={value.shape}")
                    else:
                        print(f"     {key}: éå¼ é‡={type(value)}")
                should_add_to_pool = False
                # å‰å‘æ’å¸§ï¼šè´¨é‡è¯„åˆ†åˆ¤æ–­ (PSNRå·®å€¼)
                # å½“PSNRå·®å€¼åœ¨é…ç½®çš„èŒƒå›´å†…æ—¶ï¼Œè®¤ä¸ºè´¨é‡åˆæ ¼
                if (quality_score < cfg.interp_quality_psnr_max) and (quality_score > cfg.interp_quality_psnr_min):
                    should_add_to_pool = True
                    enhanced_samples.append(sample)
                    print(f"   âœ… å‰å‘æ’å¸§è´¨é‡åˆæ ¼ (PSNRå·®å€¼={quality_score:.4f}ï¼ŒèŒƒå›´ {cfg.interp_quality_psnr_min}~{cfg.interp_quality_psnr_max})ï¼ŒåŠ å…¥æ’å€¼æ± ")
                else:
                    print(f"   âŒ å‰å‘æ’å¸§è´¨é‡ä¸åˆæ ¼ (PSNRå·®å€¼={quality_score:.4f}ï¼Œè¶…å‡ºèŒƒå›´ {cfg.interp_quality_psnr_min}~{cfg.interp_quality_psnr_max})ï¼Œä¸åŠ å…¥æ’å€¼æ± ")
                
                # åªæœ‰è´¨é‡åˆæ ¼çš„æ’å¸§æ‰æ·»åŠ åˆ°å¯ç”¨æ’å€¼è§†è§’æ± 
                if should_add_to_pool:
                    self.available_interpolation_views.append({
                        "pose": interpolated_pose_device,  # [4, 4] - ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                        "K": interp_K_device,  # [3, 3] - ä½¿ç”¨æ’å€¼è§†è§’çš„å†…å‚
                        "image_id": interp_img_id_device,  # ä½¿ç”¨æ’å€¼è§†è§’çš„å›¾åƒID
                        "enhanced_image": enhanced_interp,
                        "source": f"interpolated_step_{step}_view_{i}",
                        "quality_score": quality_score,  # è®°å½•è´¨é‡è¯„åˆ†
                        "direction": direction  # è®°å½•æ’å¸§æ–¹å‘
                    })
                    print(f"   ğŸ¯ æ’å¸§å·²åŠ å…¥æ± ä¸­ï¼Œå½“å‰æ± å¤§å°: {len(self.available_interpolation_views)}")
                else:
                    print(f"   ğŸš« æ’å¸§æœªåŠ å…¥æ± ä¸­ï¼Œå½“å‰æ± å¤§å°: {len(self.available_interpolation_views)}")
                
                print(f"   âœ… æ’å€¼å¸§ {i+1}/{cfg.virtual_view_poses_per_step} å¤„ç†å®Œæˆ (Î±={alpha:.3f}, å‚è€ƒè®­ç»ƒè§†è§’={nearest_train_idx})")
                print(f"   ğŸ” è™šæ‹Ÿè§†è§’æ± æ›´æ–°: å½“å‰æœ‰ {len(self.available_interpolation_views)} ä¸ªè§†è§’")
            
        except Exception as e:
            print(f"âŒ è™šæ‹Ÿè§†è§’æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
            return []
        
        # æ‰“å°å¤„ç†ç»“æœ
        if enhanced_samples:
            print(f"ğŸ¯ æ­¥æ•° {step} è™šæ‹Ÿè§†è§’æ‰¹æ¬¡å¤„ç†å®Œæˆï¼")
            print(f"   æˆåŠŸç”Ÿæˆ {len(enhanced_samples)} ä¸ªå¢å¼ºè§†è§’")
            print(f"   æ’å€¼è§†è§’æ± ç°åŒ…å« {len(self.available_interpolation_views)} ä¸ªè§†è§’")
        else:
            print(f"âš ï¸ æ­¥æ•° {step} è™šæ‹Ÿè§†è§’æ‰¹æ¬¡å¤„ç†å¤±è´¥ï¼Œæ²¡æœ‰æˆåŠŸç”Ÿæˆçš„è§†è§’")
        
        return enhanced_samples
    
    def _find_nearest_training_view(self, target_pose: torch.Tensor, trainset) -> int:
        """
        æ‰¾åˆ°ä¸ç›®æ ‡poseæœ€è¿‘çš„è®­ç»ƒè§†è§’
        
        Args:
            target_pose: ç›®æ ‡pose [4, 4]
            trainset: è®­ç»ƒæ•°æ®é›†
            
        Returns:
            æœ€è¿‘è®­ç»ƒè§†è§’çš„ç´¢å¼•
        """
        min_distance = float('inf')
        nearest_idx = 0
        
        target_position = target_pose[:3, 3]  # [3]
        
        for i in range(len(trainset)):
            train_data = trainset[i]
            train_pose = train_data["camtoworld"].to(self.device)  # [4, 4]
            train_position = train_pose[:3, 3]  # [3]
            
            # è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»
            distance = torch.norm(target_position - train_position).item()
            
            if distance < min_distance:
                min_distance = distance
                nearest_idx = i
        
        return nearest_idx
    def initialize_interpolation_pool(
        self,
        trainset,
        rasterize_splats_fn,
        cfg
    ):
        """
        ä¸€æ¬¡æ€§åˆå§‹åŒ–æ’å€¼æ± å’ŒPSNRåŸºå‡†
        
        Args:
            trainset: è®­ç»ƒæ•°æ®é›†
            rasterize_splats_fn: 3DGSæ¸²æŸ“å‡½æ•°
            cfg: é…ç½®å¯¹è±¡
        """
        if self.is_initialized:
            print("ğŸ”„ æ’å€¼æ± å·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
            return
        
        print("ğŸš€ å¼€å§‹åˆå§‹åŒ–æ’å€¼æ± å’ŒPSNRåŸºå‡†...")
        
        # 1. åˆå§‹åŒ–VirtualViewQualityScorer
        self.quality_scorer = VirtualViewQualityScorer()
        print("âœ… VirtualViewQualityScoreråˆå§‹åŒ–å®Œæˆ")
        
        # 2. å¤„ç†è®­ç»ƒè§†è§’ï¼Œè®¡ç®—å›ºå®šPSNRåŸºå‡†
        print("ğŸ“Š å¤„ç†è®­ç»ƒè§†è§’å¹¶è®¡ç®—PSNRåŸºå‡†...")
        
        # é€‰æ‹©å‰3ä¸ªè®­ç»ƒè§†è§’ä½œä¸ºåŸºå‡†
        num_training_views = min(3, len(trainset))
        all_original_views = []
        all_difix_views = []
        
        for i in range(num_training_views):
            try:
                # è·å–è®­ç»ƒè§†è§’æ•°æ®
                train_data = trainset[i]
                train_pose = train_data["camtoworld"].unsqueeze(0).to(self.device)  # [1, 4, 4]
                train_K = train_data["K"].unsqueeze(0).to(self.device)  # [1, 3, 3]
                train_image = train_data["image"].unsqueeze(0).to(self.device) / 255.0  # [1, H, W, 3]
                
                # ç¡®ä¿image_idæ˜¯å¼ é‡æ ¼å¼
                if isinstance(train_data["image_id"], int):
                    train_img_id = torch.tensor([train_data["image_id"]], device=self.device)
                else:
                    train_img_id = train_data["image_id"].unsqueeze(0).to(self.device)
                
                height, width = train_image.shape[1:3]
                
                # æ¸²æŸ“è®­ç»ƒè§†è§’
                renders_train, _, _ = rasterize_splats_fn(
                    camtoworlds=train_pose,
                    Ks=train_K,
                    width=width,
                    height=height,
                    sh_degree=cfg.sh_degree,
                    near_plane=cfg.near_plane,
                    far_plane=cfg.far_plane,
                    image_ids=train_img_id,
                    render_mode="RGB",
                )
                
                # ä½¿ç”¨DiFix3Då¢å¼ºè®­ç»ƒè§†è§’
                print(f"   ğŸ¨ å¼€å§‹DiFix3Då¤„ç†è®­ç»ƒè§†è§’ {i+1}...")
                
                # ä¸ºè®­ç»ƒè§†è§’é€‰æ‹©å‚è€ƒå›¾åƒï¼šä½¿ç”¨å¦ä¸€ä¸ªè®­ç»ƒè§†è§’çš„æ¸²æŸ“ä½œä¸ºå‚è€ƒ
                ref_image_for_training = None
                if cfg.difix3d_use_ref_image:
                    # é€‰æ‹©å¦ä¸€ä¸ªè®­ç»ƒè§†è§’çš„åŸå§‹æ¸²æŸ“ä½œä¸ºå‚è€ƒ
                    ref_idx = (i + 1) % num_training_views
                    if ref_idx != i:  # ç¡®ä¿ä¸æ˜¯åŒä¸€ä¸ªè§†è§’
                        # æ¸²æŸ“å‚è€ƒè§†è§’
                        ref_train_data = trainset[ref_idx]
                        ref_train_pose = ref_train_data["camtoworld"].unsqueeze(0).to(self.device)
                        ref_train_K = ref_train_data["K"].unsqueeze(0).to(self.device)
                        
                        # ç¡®ä¿image_idæ˜¯å¼ é‡æ ¼å¼
                        if isinstance(ref_train_data["image_id"], int):
                            ref_train_img_id = torch.tensor([ref_train_data["image_id"]], device=self.device)
                        else:
                            ref_train_img_id = ref_train_data["image_id"].unsqueeze(0).to(self.device)
                        
                        # æ¸²æŸ“å‚è€ƒè§†è§’
                        ref_renders, _, _ = rasterize_splats_fn(
                            camtoworlds=ref_train_pose,
                            Ks=ref_train_K,
                            width=width,  # ä½¿ç”¨å½“å‰è§†è§’çš„å°ºå¯¸
                            height=height,
                            sh_degree=cfg.sh_degree,
                            near_plane=cfg.near_plane,
                            far_plane=cfg.far_plane,
                            image_ids=ref_train_img_id,
                            render_mode="RGB",
                        )
                        
                        ref_image_for_training = ref_renders[0].to(self.device)  # [H, W, 3] - ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                        print(f"   ğŸ“· ä½¿ç”¨è®­ç»ƒè§†è§’ {ref_idx+1} çš„åŸå§‹æ¸²æŸ“ä½œä¸ºå‚è€ƒå›¾åƒ")
                    else:
                        print(f"   ğŸš« æ— æ³•é€‰æ‹©ä¸åŒçš„è®­ç»ƒè§†è§’ä½œä¸ºå‚è€ƒï¼Œè·³è¿‡å‚è€ƒå›¾åƒ")
                else:
                    print(f"   ğŸš« ä¸ä½¿ç”¨å‚è€ƒå›¾åƒè¿›è¡ŒDiFix3Då¤„ç†")
                
                enhanced_train = self.process_image(
                    renders_train[0],  # [H, W, 3]
                    prompt=cfg.difix3d_prompt,
                    num_inference_steps=cfg.difix3d_num_inference_steps,
                    timesteps=[199],
                    guidance_scale=cfg.difix3d_guidance_scale,
                    ref_image=ref_image_for_training,  # ä½¿ç”¨é€‰æ‹©çš„å‚è€ƒå›¾åƒ
                    save_comparison=False
                )
                print(f"   ğŸ¨ DiFix3Då¤„ç†å®Œæˆ")
                
                # æ”¶é›†ç”¨äºPSNRè®¡ç®—çš„æ•°æ®
                all_original_views.append(renders_train[0])
                all_difix_views.append(enhanced_train)
                
                # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥å›¾åƒæ˜¯å¦ç›¸åŒ
                print(f"   ğŸ” è®­ç»ƒè§†è§’ {i+1} è°ƒè¯•ä¿¡æ¯:")
                print(f"     åŸå§‹å›¾åƒå½¢çŠ¶: {renders_train[0].shape}, èŒƒå›´: [{renders_train[0].min():.4f}, {renders_train[0].max():.4f}]")
                print(f"     DiFixå›¾åƒå½¢çŠ¶: {enhanced_train.shape}, èŒƒå›´: [{enhanced_train.min():.4f}, {enhanced_train.max():.4f}]")
                
                # è®¡ç®—MSEæ¥æ£€æŸ¥å›¾åƒå·®å¼‚
                mse = torch.mean((renders_train[0] - enhanced_train) ** 2)
                print(f"     å›¾åƒMSE: {mse.item():.8f}")
                
                if mse < 1e-8:
                    print(f"     âš ï¸ è­¦å‘Šï¼šåŸå§‹å›¾åƒå’ŒDiFixå›¾åƒå‡ ä¹å®Œå…¨ç›¸åŒï¼")
                    print(f"       è¿™å¯èƒ½æ„å‘³ç€DiFix3Då¤„ç†æ²¡æœ‰ç”Ÿæ•ˆ")
                else:
                    print(f"     âœ… å›¾åƒæœ‰å·®å¼‚ï¼ŒDiFix3Då¤„ç†ç”Ÿæ•ˆ")
                
                # ç›´æ¥æ·»åŠ åˆ°å¯ç”¨æ’å€¼è§†è§’æ± ï¼ˆè®­ç»ƒè§†è§’æ— éœ€è¯„åˆ†ï¼‰
                self.available_interpolation_views.append({
                    "pose": train_pose[0],  # [4, 4]
                    "K": train_K[0],  # [3, 3]
                    "image_id": train_img_id[0],
                    "enhanced_image": enhanced_train,
                    "source": f"training_view_{i}"
                })
                
                print(f"   âœ… è®­ç»ƒè§†è§’ {i+1}/{num_training_views} å¤„ç†å®Œæˆ")
                
            except Exception as e:
                print(f"   âŒ è®­ç»ƒè§†è§’ {i} å¤„ç†å¤±è´¥: {e}")
                continue
        
        if len(all_original_views) == 0:
            print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„è®­ç»ƒè§†è§’ï¼Œæ— æ³•è®¡ç®—PSNRåŸºå‡†")
            return
        
        # 3. è®¡ç®—å›ºå®šPSNRåŸºå‡†ï¼ˆä¸ä¼šåœ¨åç»­è¿‡ç¨‹ä¸­æ›´æ–°ï¼‰
        try:
            self.training_psnr_mean, self.training_psnr_variance = self.quality_scorer.evaluate_training_views(
                all_original_views, all_difix_views
            )
            
            # æ£€æŸ¥PSNRå€¼çš„æœ‰æ•ˆæ€§
            if np.isinf(self.training_psnr_mean) or np.isnan(self.training_psnr_mean):
                print(f"âŒ PSNRå‡å€¼ä¸ºæ— æ•ˆå€¼: {self.training_psnr_mean}")
                print(f"   è¿™é€šå¸¸æ„å‘³ç€DiFix3Då¤„ç†åçš„å›¾åƒä¸åŸå§‹å›¾åƒå®Œå…¨ç›¸åŒ")
                print(f"   è¯·æ£€æŸ¥DiFix3Då¤„ç†æ˜¯å¦æ­£å¸¸å·¥ä½œ")
                raise ValueError("PSNRå‡å€¼ä¸ºinfï¼ŒDiFix3Då¤„ç†å¯èƒ½æ²¡æœ‰ç”Ÿæ•ˆ")
            
            if np.isinf(self.training_psnr_variance) or np.isnan(self.training_psnr_variance):
                print(f"âŒ PSNRæ–¹å·®ä¸ºæ— æ•ˆå€¼: {self.training_psnr_variance}")
                print(f"   è¿™é€šå¸¸æ„å‘³ç€æ‰€æœ‰å›¾åƒçš„PSNRå€¼éƒ½ç›¸åŒï¼ˆéƒ½æ˜¯infï¼‰")
                raise ValueError("PSNRæ–¹å·®ä¸ºnanï¼Œæ‰€æœ‰å›¾åƒå¯èƒ½å®Œå…¨ç›¸åŒ")
            
            print(f"ğŸ“Š å›ºå®šPSNRåŸºå‡†è®¡ç®—å®Œæˆ: å‡å€¼={self.training_psnr_mean:.4f}, æ–¹å·®={self.training_psnr_variance:.4f}")
            
            # ä¿å­˜åŸºç¡€æ‰“åˆ†æ•°æ®
            self.baseline_scores = {
                "training_psnr_mean": float(self.training_psnr_mean),
                "training_psnr_variance": float(self.training_psnr_variance),
                "training_views_count": len(all_original_views),
                "timestamp": time.time()
            }
            print(f"ğŸ’¾ å·²ä¿å­˜åŸºç¡€æ‰“åˆ†æ•°æ®: å‡å€¼={self.training_psnr_mean:.4f}, æ–¹å·®={self.training_psnr_variance:.4f}")
            
        except Exception as e:
            print(f"âŒ PSNRåŸºå‡†è®¡ç®—å¤±è´¥: {e}")
            print(f"   åŸå› åˆ†æï¼š")
            print(f"   1. DiFix3Då¤„ç†å¯èƒ½æ²¡æœ‰ç”Ÿæ•ˆï¼Œè¿”å›äº†åŸå§‹å›¾åƒ")
            print(f"   2. å›¾åƒæ•°æ®å¯èƒ½æœ‰é—®é¢˜")
            print(f"   3. è¯·æ£€æŸ¥DiFix3Dæ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½")
            raise e
        print(f"ğŸ”„ å¯ç”¨æ’å€¼è§†è§’æ± åˆå§‹åŒ–å®Œæˆï¼ŒåŒ…å« {len(self.available_interpolation_views)} ä¸ªè®­ç»ƒè§†è§’")
        
        # æ ‡è®°ä¸ºå·²åˆå§‹åŒ–
        self.is_initialized = True
        print("âœ… æ’å€¼æ± å’ŒPSNRåŸºå‡†åˆå§‹åŒ–å®Œæˆ")
    

class DeblurDiFix3DRunner(Runner):
    """BAD-Gaussianså»æ¨¡ç³Š + DiFix3Dè®­ç»ƒå¼•æ“"""

    def __init__(self, local_rank: int, world_rank, world_size: int, cfg: DeblurDiFix3DConfig) -> None:
        set_random_seed(42 + local_rank)

        self.cfg = cfg
        self.world_rank = world_rank
        self.local_rank = local_rank
        self.world_size = world_size
        self.device = f"cuda:{local_rank}"

        # è®¾ç½®è¾“å‡ºç›®å½•
        self.result_dir = cfg.result_dir  # ä¿å­˜åŸå§‹è·¯å¾„
        os.makedirs(cfg.result_dir, exist_ok=True)
        self.ckpt_dir = f"{cfg.result_dir}/ckpts"
        os.makedirs(self.ckpt_dir, exist_ok=True)
        self.stats_dir = f"{cfg.result_dir}/stats"
        os.makedirs(self.stats_dir, exist_ok=True)
        self.render_dir = f"{cfg.result_dir}/renders"
        os.makedirs(self.render_dir, exist_ok=True)
        # DiFix3Då¯¹æ¯”å›¾åƒä¿å­˜ç›®å½•
        self.difix3d_comparison_dir = f"{cfg.result_dir}/difix3d_comparisons"
        os.makedirs(self.difix3d_comparison_dir, exist_ok=True)

        # Tensorboard
        self.writer = SummaryWriter(log_dir=f"{cfg.result_dir}/tb")

        # ä»data_dirä¸­æå–scene_nameï¼Œç”¨äºæ„å»ºref_imageè·¯å¾„
        self.scene_name = Path(cfg.data_dir).name
        self.ref_image_dir = f"{cfg.data_dir}/ref_image"
        print(f"ğŸ” åœºæ™¯åç§°: {self.scene_name}")
        print(f"ğŸ” å‚è€ƒå›¾åƒç›®å½•: {self.ref_image_dir}")

        # åŠ è½½æ•°æ®
        self.parser = ColmapParser(
            data_dir=cfg.data_dir,
            factor=cfg.data_factor,  # å¼ºåˆ¶ä½¿ç”¨åŸå§‹å›¾åƒï¼Œä¸ä½¿ç”¨ä»»ä½•ä¸‹é‡‡æ ·
            normalize=True,
            scale_factor=cfg.scale_factor,
            # å¼ºåˆ¶ç¦ç”¨è‡ªåŠ¨ä¸‹é‡‡æ ·
            downscale_rounding_mode="round",  # ä½¿ç”¨roundè€Œä¸æ˜¯floor
        )
        # è®­ç»ƒç´¢å¼•é…ç½®ï¼šå°† CLI/é…ç½®ä¸­çš„ train_indices ä¼ é€’ç»™è§£æå™¨
        self.parser.train_indices = cfg.train_indices
        if cfg.train_indices is not None:
            print(f"[Dataset] ä½¿ç”¨é…ç½®çš„è®­ç»ƒç´¢å¼•: {cfg.train_indices}")
        
        # è°ƒè¯•ï¼šæ£€æŸ¥ColmapParserçš„é…ç½®
        print(f"ğŸ” ColmapParseré…ç½®æ£€æŸ¥:")
        print(f"   data_factor: {cfg.data_factor}")
        print(f"   scale_factor: {cfg.scale_factor}")
        print(f"   downscale_rounding_mode: {self.parser.downscale_rounding_mode}")
        print(f"   parser.factor: {self.parser.factor}")
        if hasattr(self.parser, '_downscale_factor'):
            print(f"   parser._downscale_factor: {self.parser._downscale_factor}")
        
        # æ£€æŸ¥å›¾åƒè·¯å¾„
        print(f"ğŸ” å›¾åƒè·¯å¾„æ£€æŸ¥:")
        if hasattr(self.parser, 'image_paths') and len(self.parser.image_paths) > 0:
            sample_path = Path(self.parser.image_paths[0])
            print(f"   æ ·æœ¬å›¾åƒè·¯å¾„: {sample_path}")
            if sample_path.exists():
                img = Image.open(sample_path)
                print(f"   æ ·æœ¬å›¾åƒå°ºå¯¸: {img.size}")
            else:
                print(f"   âš ï¸ æ ·æœ¬å›¾åƒè·¯å¾„ä¸å­˜åœ¨!")
        else:
            print(f"   âš ï¸ æ²¡æœ‰æ‰¾åˆ°å›¾åƒè·¯å¾„!")

        self.trainset = DeblurNerfDataset(self.parser, split="train")
        
        # åˆå§‹åŒ–ç›¸æœºè½¨è¿¹ç”Ÿæˆå™¨
        self.trajectory_generator = generate_camera_trajectory
        self.valset = DeblurNerfDataset(self.parser, split="val")
        self.testset = DeblurNerfDataset(self.parser, split="test")
        self.quality_scorer = VirtualViewQualityScorer(device=self.device)
        print(f"âœ… è™šæ‹Ÿè§†è§’è´¨é‡æ‰“åˆ†æ¨¡å‹å·²åˆå§‹åŒ–")
        # åˆå§‹åŒ–DiFix3Då¤„ç†å™¨
        if cfg.enable_difix3d:
            print("ğŸ¨ åˆå§‹åŒ–DiFix3Då¤„ç†å™¨...")
            self.difix3d_processor = DiFix3DProcessor(
                model_name=cfg.difix3d_model_name,
                device=self.device,
                ref_image_dir=self.ref_image_dir
            )
            if self.difix3d_processor.enabled:
                print(f"âœ… DiFix3Då¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                print(f"âš ï¸ DiFix3Då¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ç¦ç”¨DiFix3DåŠŸèƒ½")
                cfg.enable_difix3d = False
        else:
            self.difix3d_processor = None
            print("ğŸš« DiFix3DåŠŸèƒ½å·²ç¦ç”¨")

        self.scene_scale = self.parser.scene_scale * 1.1 * cfg.global_scale

        # åˆå§‹åŒ–3Dé«˜æ–¯ç‚¹
        feature_dim = None
        if cfg.app_opt:
            feature_dim = cfg.app_embed_dim

        self.splats, self.optimizers = create_splats_with_optimizers(
            self.parser,
            init_type=cfg.init_type,
            init_num_pts=cfg.init_num_pts,
            init_extent=cfg.init_extent,
            init_opacity=cfg.init_opa,
            init_scale=cfg.init_scale,
            scene_scale=self.scene_scale,
            sh_degree=cfg.sh_degree,
            sparse_grad=cfg.sparse_grad,
            batch_size=cfg.batch_size,
            feature_dim=feature_dim,
            device=self.device,
            world_rank=world_rank,
            world_size=world_size,
        )
        print("æ¨¡å‹åˆå§‹åŒ–å®Œæˆ. é«˜æ–¯ç‚¹æ•°é‡:", len(self.splats["means"]))

        # å¯†é›†åŒ–ç­–ç•¥
        self.cfg.strategy.check_sanity(self.splats, self.optimizers)

        if isinstance(self.cfg.strategy, DefaultStrategy):
            self.strategy_state = self.cfg.strategy.initialize_state(scene_scale=self.scene_scale)
        elif isinstance(self.cfg.strategy, MCMCStrategy):
            self.strategy_state = self.cfg.strategy.initialize_state()
        else:
            assert_never(self.cfg.strategy)

        # BAD-Gaussiansç›¸æœºä¼˜åŒ–å™¨
        self.pose_optimizers = []
        # è®¡ç®—æ€»ç›¸æœºæ•°é‡ï¼ŒåŒ…æ‹¬è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
        total_cameras = len(self.trainset) + (len(self.valset) if self.valset else 0) + (len(self.testset) if self.testset else 0)
        self.camera_optimizer: BadCameraOptimizer = self.cfg.camera_optimizer.setup(
            num_cameras=total_cameras,
            device=self.device,
        )
        camera_optimizer_param_groups = {}
        # å¤„ç†DDPåŒ…è£…çš„æƒ…å†µ
        camera_optimizer = self.camera_optimizer.module if hasattr(self.camera_optimizer, 'module') else self.camera_optimizer
        camera_optimizer.get_param_groups(camera_optimizer_param_groups)
        self.pose_optimizers = [
            torch.optim.Adam(
                camera_optimizer_param_groups["camera_opt"],
                lr=cfg.pose_opt_lr * math.sqrt(cfg.batch_size),
                weight_decay=cfg.pose_opt_reg,
            )
        ]
        if world_size > 1:
            self.camera_optimizer = DDP(self.camera_optimizer)

        # å¤–è§‚ä¼˜åŒ–å™¨
        self.app_optimizers = []
        if cfg.app_opt:
            assert feature_dim is not None
            self.app_module = AppearanceOptModule(
                len(self.trainset), feature_dim, cfg.app_embed_dim, cfg.sh_degree
            ).to(self.device)
            torch.nn.init.zeros_(self.app_module.color_head[-1].weight)
            torch.nn.init.zeros_(self.app_module.color_head[-1].bias)
            self.app_optimizers = [
                torch.optim.Adam(
                    self.app_module.embeds.parameters(),
                    lr=cfg.app_opt_lr * math.sqrt(cfg.batch_size) * 10.0,
                ),
                torch.optim.Adam(
                    self.app_module.color_head.parameters(),
                    lr=cfg.app_opt_lr * math.sqrt(cfg.batch_size),
                ),
            ]
            if world_size > 1:
                self.app_module = DDP(self.app_module)

        # åŒè¾¹ç½‘æ ¼
        self.bil_grid_optimizers = []
        if cfg.use_bilateral_grid:
            self.bil_grids = BilateralGrid(
                len(self.trainset),
                grid_X=cfg.bilateral_grid_shape[0],
                grid_Y=cfg.bilateral_grid_shape[1], 
                grid_W=cfg.bilateral_grid_shape[2],
            ).to(self.device)
            self.bil_grid_optimizers = [
                torch.optim.Adam(
                    self.bil_grids.parameters(),
                    lr=2e-3 * math.sqrt(cfg.batch_size),
                    eps=1e-15,
                ),
            ]

        # è¯„ä¼°æŒ‡æ ‡
        self.psnr = PeakSignalNoiseRatio(data_range=1.0).to(self.device)
        self.ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(self.device)
        self.lpips = LearnedPerceptualImagePatchSimilarity(normalize=True).to(self.device)
        
        # åˆå§‹åŒ–VGGæ„ŸçŸ¥æŸå¤±æ¨¡å‹ï¼ˆé¢„åˆ›å»ºï¼Œé¿å…é‡å¤åˆ›å»ºï¼‰
        self.perceptual_loss = VGG16PerceptualLoss(
            feature_layer='relu2_2',
            device=self.device,
            enable_timing=False
        )
        self.dists_loss = VGG16DISTSLoss(
            device=self.device,
            enable_timing=False
        )

        # åˆå§‹åŒ–æŸ¥çœ‹å™¨
        if not cfg.disable_viewer:
            import nerfview
            self.server = viser.ViserServer(port=cfg.port, verbose=False)
            self.viewer = nerfview.Viewer(
                server=self.server,
                render_fn=self._viewer_render_fn,
                mode="training",
            )

        self.cfg_to_save = cfg
        
        # ç”¨äºå­˜å‚¨è™šæ‹Ÿç›¸æœºä½ç½®æ•°æ®
        self.virtual_camera_batches = []
        # ç”¨äºå­˜å‚¨æ‰€æœ‰è®­ç»ƒç›¸æœºä½ç½®æ•°æ®
        self.all_train_cameras = None
        
        # ç”¨äºå­˜å‚¨è™šæ‹Ÿè§†è§’è´¨é‡è¯„åˆ†æ•°æ®
        self.virtual_view_scores = []
        # ç”¨äºå­˜å‚¨åŸºç¡€æ‰“åˆ†æ•°æ®ï¼ˆè®­ç»ƒè§†è§’PSNRåŸºå‡†ï¼‰
        self.baseline_scores = {}
        
        # åˆå§‹åŒ–æ··åˆé‡‡æ ·ç­–ç•¥ï¼ˆæŒ‰éœ€æ’å¸§æ¨¡å¼ï¼‰
        # æ··åˆé‡‡æ ·ç­–ç•¥çŠ¶æ€è·Ÿè¸ª
        self.hybrid_sampling_initialized = False
        
    def collect_train_camera_data(self):
        """
        æ”¶é›†æ‰€æœ‰è®­ç»ƒç›¸æœºä½ç½®æ•°æ®
        """
        if self.all_train_cameras is None:
            train_cameras = []
            for i in range(len(self.trainset)):
                camera_info = self.trainset[i]
                if 'camtoworld' in camera_info:
                    train_cameras.append(camera_info['camtoworld'])
                elif 'pose' in camera_info:
                    train_cameras.append(camera_info['pose'])
            
            if train_cameras:
                self.all_train_cameras = torch.stack(train_cameras).to(self.device)  # [N, 4, 4]
                print(f"ğŸ“Š æ”¶é›†åˆ° {len(train_cameras)} ä¸ªè®­ç»ƒç›¸æœºä½ç½®")
            else:
                print("âš ï¸ æ— æ³•ä»è®­ç»ƒæ•°æ®é›†è·å–ç›¸æœºä½ç½®")


    def collect_virtual_camera_data(self, camera_poses: torch.Tensor = None, enhanced_samples: List[dict] = None, step: int = None, source: str = "unknown"):
        """
        æ”¶é›†è™šæ‹Ÿç›¸æœºä½ç½®æ•°æ®ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
        
        Args:
            camera_poses: ç›¸æœºposes [N, 4, 4] (BAD-Gaussiansä½¿ç”¨)
            enhanced_samples: å¢å¼ºæ ·æœ¬åˆ—è¡¨ (DiFix3Dä½¿ç”¨)
            step: å½“å‰æ­¥æ•°ï¼ˆå¯é€‰ï¼‰
            source: æ•°æ®æ¥æºï¼ˆ"BAD-Gaussians" æˆ– "DiFix3D"ï¼‰
        """
        if camera_poses is not None and len(camera_poses) > 0:
            # BAD-Gaussiansè™šæ‹Ÿç›¸æœº
            self.virtual_camera_batches.append(camera_poses.detach().clone())
            step_info = f"æ­¥æ•°{step}: " if step is not None else ""
            print(f"ğŸ“Š {step_info}æ”¶é›†åˆ°{source}è™šæ‹Ÿç›¸æœº {len(camera_poses)} ä¸ª")
        elif enhanced_samples:
            # DiFix3Då¢å¼ºè™šæ‹Ÿç›¸æœº
            virtual_poses = []
            print(f"ğŸ” è°ƒè¯•enhanced_samplesè®¾å¤‡ä¿¡æ¯:")
            for i, sample in enumerate(enhanced_samples):
                pose = sample["pose"]  # [4, 4] - ä¸åŒ…å«batchç»´åº¦
                print(f"   æ ·æœ¬{i}: poseè®¾å¤‡={pose.device}, å½¢çŠ¶={pose.shape}, æœŸæœ›è®¾å¤‡={self.device}")
                # ç¡®ä¿poseåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                if pose.device != self.device:
                    print(f"   ğŸ”§ æ ·æœ¬{i}: å°†poseä»{pose.device}ç§»åŠ¨åˆ°{self.device}")
                    pose = pose.to(self.device)
                virtual_poses.append(pose.unsqueeze(0))  # æ·»åŠ batchç»´åº¦ [1, 4, 4]
            
            if virtual_poses:
                print(f"ğŸ” è°ƒè¯•virtual_posesè®¾å¤‡ä¿¡æ¯:")
                for i, pose in enumerate(virtual_poses):
                    print(f"   virtual_poses[{i}]: è®¾å¤‡={pose.device}, å½¢çŠ¶={pose.shape}")
                
                try:
                    virtual_cameras_batch = torch.cat(virtual_poses, dim=0)  # [N, 4, 4]
                    self.virtual_camera_batches.append(virtual_cameras_batch)
                    print(f"ğŸ“Š æ”¶é›†åˆ°{source}è™šæ‹Ÿç›¸æœº {len(virtual_poses)} ä¸ª")
                    print(f"   ğŸ“Š å½“å‰æ€»è™šæ‹Ÿç›¸æœºæ‰¹æ¬¡æ•°é‡: {len(self.virtual_camera_batches)}")
                    total_virtual_cameras = sum(len(batch) for batch in self.virtual_camera_batches)
                    print(f"   ğŸ“Š å½“å‰æ€»è™šæ‹Ÿç›¸æœºæ•°é‡: {total_virtual_cameras}")
                except Exception as e:
                    print(f"âŒ torch.catå¤±è´¥: {e}")
                    print(f"   æ‰€æœ‰å¼ é‡è®¾å¤‡: {[pose.device for pose in virtual_poses]}")
                    raise e


    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯ - å®Œå…¨åŸºäºsimple_trainer_deblur.py"""
        cfg = self.cfg
        device = self.device
        world_rank = self.world_rank
        world_size = self.world_size

        # Dump cfg.
        if world_rank == 0:
            with open(f"{cfg.result_dir}/cfg.yml", "w") as f:
                yaml.dump(vars(cfg), f)

        max_steps = cfg.max_steps
        init_step = 0

        schedulers = [
            # means has a learning rate schedule, that end at 0.01 of the initial value
            torch.optim.lr_scheduler.ExponentialLR(self.optimizers["means"], gamma=0.01 ** (1.0 / max_steps)),
        ]
        
        # pose optimization has a learning rate schedule
        pose_scheduler = torch.optim.lr_scheduler.ExponentialLR(
            self.pose_optimizers[0], gamma=cfg.pose_opt_lr_decay ** (1.0 / max_steps)
        )
        schedulers.append(pose_scheduler)

        if cfg.use_bilateral_grid:
            # bilateral grid has a learning rate schedule. Linear warmup for 1000 steps.
            schedulers.append(
                torch.optim.lr_scheduler.ChainedScheduler(
                    [
                        torch.optim.lr_scheduler.LinearLR(
                            self.bil_grid_optimizers[0],
                            start_factor=0.01,
                            total_iters=1000,
                        ),
                        torch.optim.lr_scheduler.ExponentialLR(
                            self.bil_grid_optimizers[0], gamma=0.01 ** (1.0 / max_steps)
                        ),
                    ]
                )
            )

        trainloader = torch.utils.data.DataLoader(
            self.trainset,
            batch_size=cfg.batch_size,
            shuffle=True,
            num_workers=4,
            persistent_workers=True,
            pin_memory=cfg.pin_memory,
        )
        trainloader_iter = iter(trainloader)

        if cfg.visualize_cameras:
            self._init_viewer_state()

        # åœ¨è®­ç»ƒå¼€å§‹å‰æ”¶é›†è®­ç»ƒç›¸æœºæ•°æ®
        if world_rank == 0:
            print("ğŸ“Š å¼€å§‹æ”¶é›†è®­ç»ƒç›¸æœºä½ç½®æ•°æ®...")
            self.collect_train_camera_data()

        # Training loop.
        global_tic = time.time()
        pbar = tqdm.tqdm(range(init_step, max_steps))
        for step in pbar:
            if not cfg.disable_viewer:
                while self.viewer.state.status == "paused":
                    time.sleep(0.01)
                self.viewer.lock.acquire()
                tic = time.time()

            try:
                data = next(trainloader_iter)
            except StopIteration:
                trainloader_iter = iter(trainloader)
                data = next(trainloader_iter)
                
            camtoworlds = camtoworlds_gt = data["camtoworld"].to(device, non_blocking=True)  # [1, 4, 4]
            Ks = data["K"].to(device, non_blocking=True)  # [1, 3, 3]
            pixels = data["image"].to(device, non_blocking=True) / 255.0  # [1, H, W, 3]
            
            num_train_rays_per_step = pixels.shape[0] * pixels.shape[1] * pixels.shape[2]
            image_ids = data["image_id"].to(device, non_blocking=True)
            if cfg.depth_loss:
                points = data["points"].to(device, non_blocking=True)  # [1, M, 2]
                depths_gt = data["depths"].to(device, non_blocking=True)  # [1, M]

            height, width = pixels.shape[1:3]

            assert camtoworlds.shape[0] == 1
            # å¤„ç†DDPåŒ…è£…çš„æƒ…å†µ
            camera_optimizer = self.camera_optimizer.module if hasattr(self.camera_optimizer, 'module') else self.camera_optimizer
            camtoworlds = camera_optimizer.apply_to_cameras(camtoworlds, image_ids, "uniform")[0]
            assert camtoworlds.shape[0] == cfg.camera_optimizer.num_virtual_views
            Ks = Ks.tile((camtoworlds.shape[0], 1, 1))
            
            # ğŸ“Š æ³¨é‡Šæ‰BAD-Gaussiansè™šæ‹Ÿç›¸æœºæ”¶é›†ï¼Œåªä¿ç•™DiFix3Dçš„è™šæ‹Ÿç›¸æœº
            # if step % 1000 == 0:  # æ¯1000æ­¥æ”¶é›†ä¸€æ¬¡ï¼Œé¿å…æ•°æ®è¿‡å¤š
            #     self.collect_virtual_camera_data(camera_poses=camtoworlds, step=step, source="BAD-Gaussians")

            
            sh_degree_to_use = min(step // cfg.sh_degree_interval, cfg.sh_degree)

            
            renders, alphas, info = self.rasterize_splats(
                camtoworlds=camtoworlds,
                Ks=Ks,
                width=width,
                height=height,
                sh_degree=sh_degree_to_use,
                near_plane=cfg.near_plane,
                far_plane=cfg.far_plane,
                image_ids=image_ids,
                render_mode="RGB+ED" if (cfg.depth_loss or cfg.enable_depth_smooth_loss) else "RGB",
            )
            
            if renders.shape[-1] == 4:
                colors, depths = renders[..., 0:3], renders[..., 3:4]
            else:
                colors, depths = renders, None

            if cfg.random_bkgd:
                bkgd = torch.rand(1, 3, device=device)
                colors = colors + bkgd * (1.0 - alphas)
            
            # ğŸ¯ è®¡ç®—æ·±åº¦å¹³æ»‘æŸå¤± (æ¯ä¸ªstepéƒ½æ‰§è¡Œ)
            depth_smooth_loss_value = 0.0

            # BAD-Gaussians: average the virtual views
            colors = colors.mean(0)[None]
            
            # ğŸ¯ è™šæ‹Ÿè§†è§’è®­ç»ƒç­–ç•¥
            virtual_view_loss_to_add = 0.0  

            # ğŸ†• æ··åˆé‡‡æ ·ç­–ç•¥ï¼šç»Ÿä¸€çš„è™šæ‹Ÿè§†è§’è®­ç»ƒ
            print(f"ğŸ” æ£€æŸ¥æ··åˆé‡‡æ ·æ¡ä»¶: step={step}, virtual_view_start_step={cfg.virtual_view_start_step}, enable_difix3d={cfg.enable_difix3d}, difix3d_processor={self.difix3d_processor is not None}, step%interval={step % cfg.virtual_view_interval}")
            if step >= cfg.virtual_view_start_step and cfg.enable_difix3d and self.difix3d_processor is not None and step % cfg.virtual_view_interval == 0:
                if step == cfg.virtual_view_start_step:
                    print(f"ğŸ¯ æ­¥æ•° {step}: é¦–æ¬¡å¯ç”¨æ··åˆé‡‡æ ·ç­–ç•¥è™šæ‹Ÿè§†è§’è®­ç»ƒ")
                else:
                    print(f"ğŸ¯ æ­¥æ•° {step}: ç»§ç»­æ··åˆé‡‡æ ·ç­–ç•¥è™šæ‹Ÿè§†è§’è®­ç»ƒ")
                
                # ğŸ†• ä½¿ç”¨æ–°çš„è™šæ‹Ÿè§†è§’æ‰¹æ¬¡å¤„ç†ç­–ç•¥
                enhanced_samples = self.difix3d_processor.process_virtual_views_batch(
                    trainset=self.trainset,
                    camera_optimizer=self.camera_optimizer,
                    rasterize_splats_fn=self.rasterize_splats,
                    cfg=cfg,
                    step=step,
                    save_comparisons=cfg.difix3d_save_comparisons,
                    comparison_dir=self.difix3d_comparison_dir
                )
                
                if enhanced_samples:
                    # ğŸ¯ å°†å¤šä¸ªå¢å¼ºæ ·æœ¬æ·»åŠ åˆ°ç±»å±æ€§ä¸­
                    if not hasattr(self, 'enhanced_data'):
                        self.enhanced_data = []
                    elif not isinstance(self.enhanced_data, list):
                        self.enhanced_data = []  # é‡ç½®ä¸ºåˆ—è¡¨æ ¼å¼
                    
                    # é€ä¸ªæ·»åŠ æ–°æ ·æœ¬ï¼Œé™åˆ¶æ€»æ•°é‡
                    max_samples = getattr(cfg, 'difix3d_max_augmented_samples', 100)
                    for enhanced_sample in enhanced_samples:
                        # é™åˆ¶å¢å¼ºæ•°æ®æ•°é‡ï¼Œä¿æŒæœ€æ–°çš„æ ·æœ¬
                        if len(self.enhanced_data) >= max_samples:
                            self.enhanced_data.pop(0)  # ç§»é™¤æœ€æ—§çš„æ ·æœ¬
                        
                        self.enhanced_data.append(enhanced_sample)
                    
                    # ğŸ“Š æ”¶é›†è™šæ‹Ÿç›¸æœºä½ç½®æ•°æ®
                    self.collect_virtual_camera_data(enhanced_samples=enhanced_samples, source="DiFix3D-Progressive")
                    
                    print(f"ğŸ¯ æ¸è¿›å¼æ’å€¼å®Œæˆ:")
                    print(f"   æœ¬æ¬¡ç”Ÿæˆæ ·æœ¬æ•°: {len(enhanced_samples)}")
                    print(f"   å½“å‰æ€»å¢å¼ºæ ·æœ¬æ•°: {len(self.enhanced_data)}")
                    print(f"   ğŸ“Š å½“å‰è™šæ‹Ÿç›¸æœºæ‰¹æ¬¡æ•°é‡: {len(self.virtual_camera_batches)}")
                    total_virtual_cameras = sum(len(batch) for batch in self.virtual_camera_batches)
                    print(f"   ğŸ“Š å½“å‰è™šæ‹Ÿç›¸æœºæ€»æ•°é‡: {total_virtual_cameras}")
                    for i, sample in enumerate(enhanced_samples):
                        quality_score = sample.get('quality_score', 'N/A')
                        if isinstance(quality_score, (int, float)):
                            print(f"   æ ·æœ¬ {i}: å›¾åƒID={sample['image_id'].item()}, è´¨é‡è¯„åˆ†={quality_score:.4f}")
                        else:
                            print(f"   æ ·æœ¬ {i}: å›¾åƒID={sample['image_id'].item()}, è´¨é‡è¯„åˆ†={quality_score}")
                else:
                    print("âš ï¸ æ¸è¿›å¼æ’å€¼å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆæ–°æ ·æœ¬")
                    
            # ğŸ¯ è™šæ‹Ÿè§†è§’è®­ç»ƒå¼€å§‹æ­¥æ•°åŠä¹‹åï¼šè®¡ç®—è™šæ‹Ÿè§†è§’Loss
            if step >= cfg.virtual_view_start_step and hasattr(self, 'enhanced_data'):
                # ğŸ†• éšæœºé€‰æ‹©ä¸€ä¸ªå¢å¼ºæ ·æœ¬è¿›è¡ŒLossè®¡ç®—ï¼ˆå‡å°‘è®¡ç®—å¼€é”€ï¼‰
                if isinstance(self.enhanced_data, list) and len(self.enhanced_data) > 0:
                    # éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬
                    import random
                    sample = random.choice(self.enhanced_data)
                    
                    loss_virtual_sample = 0.0
                    
                    # ğŸ¯ å…³é”®ï¼šé‡æ–°æ¸²æŸ“è™šæ‹Ÿè§†è§’ï¼Œè·å¾—è¿æ¥åˆ°å½“å‰3Dé«˜æ–¯åœºçš„æ¢¯åº¦
                    virtual_pose = sample["pose"].unsqueeze(0).to(device)  # [1, 4, 4]
                    virtual_K = sample["K"].unsqueeze(0).to(device)  # [1, 3, 3]
                    virtual_image_id = sample["image_id"].unsqueeze(0).to(device)
                    virtual_width = sample["width"]
                    virtual_height = sample["height"]
                    
                    # é‡æ–°æ¸²æŸ“è™šæ‹Ÿè§†è§’ï¼ˆåŒ…å«æ¢¯åº¦ï¼Œè¿æ¥åˆ°å½“å‰è®­ç»ƒæ­¥çš„3Dé«˜æ–¯åœºï¼‰
                    renders_virtual, alphas_virtual, info_virtual = self.rasterize_splats(
                        camtoworlds=virtual_pose,
                        Ks=virtual_K,
                        width=virtual_width,
                        height=virtual_height,
                        sh_degree=sh_degree_to_use,
                        near_plane=cfg.near_plane,
                        far_plane=cfg.far_plane,
                        image_ids=virtual_image_id,
                        render_mode="RGB+ED" if cfg.enable_depth_smooth_loss else "RGB",
                    )
                    
                    # æå–RGBå’Œæ·±åº¦ä¿¡æ¯
                    if renders_virtual.shape[-1] == 4:
                        colors_virtual, depths_virtual = renders_virtual[..., 0:3], renders_virtual[..., 3:4]
                    else:
                        colors_virtual, depths_virtual = renders_virtual, None
                    
                    # åº”ç”¨éšæœºèƒŒæ™¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if cfg.random_bkgd:
                        colors_virtual = colors_virtual + bkgd * (1.0 - alphas_virtual)
                    
                    # è·å–DiFixå¢å¼ºåçš„å›¾åƒä½œä¸ºç›‘ç£ä¿¡å·ï¼ˆæ— æ¢¯åº¦ï¼‰
                    enhanced_image = sample["enhanced_image"].to(device)  # [H, W, 3]
                    if enhanced_image.dim() == 3:
                        enhanced_image = enhanced_image.unsqueeze(0)  # [1, H, W, 3]
                    
                    # ğŸ¯ è®¡ç®—DiFixè’¸é¦æŸå¤±ï¼šå°†å¢å¼ºå›¾åƒçš„ä¿¡æ¯è’¸é¦åˆ°æ¸²æŸ“ç»“æœä¸­
                    difix_distillation_loss = 0.0
                    if cfg.enable_difix_enhancement_loss:
                        # L1æŸå¤±
                        difix_l1_loss = F.l1_loss(colors_virtual, enhanced_image)
                        
                        # SSIMæŸå¤±
                        difix_ssim_loss = 1.0 - self.ssim(
                            colors_virtual.permute(0, 3, 1, 2), 
                            enhanced_image.permute(0, 3, 1, 2)
                        )
                        
                        # DISTSæ„ŸçŸ¥æŸå¤±
                        difix_dists_loss = self.dists_loss(colors_virtual, enhanced_image)
                        difix_perc_loss = self.perceptual_loss(colors_virtual, enhanced_image)
                        # ç»„åˆè’¸é¦æŸå¤±
                        difix_distillation_loss = (
                            difix_l1_loss * cfg.difix_enhancement_l1_weight +
                            difix_dists_loss * 0.01
                        )
                        
                        loss_virtual_sample += difix_distillation_loss
                    
                    # æ·±åº¦å¹³æ»‘æŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if cfg.enable_depth_smooth_loss and depths_virtual is not None:
                        depth_smooth_loss_virtual = depth_smooth_loss_4neighbor(depths_virtual)
                        loss_virtual_sample += depth_smooth_loss_virtual * cfg.depth_smooth_lambda
                    
                    # åº”ç”¨æƒé‡
                    virtual_view_loss_to_add = cfg.virtual_view_loss_weight * loss_virtual_sample
                    
                    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                    if step % 100 == 0:  # æ¯100æ­¥æ‰“å°ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
                        print(f"ğŸ” è™šæ‹Ÿè§†è§’Lossè°ƒè¯• (æ­¥æ•°{step}):")
                        print(f"   å¯ç”¨æ ·æœ¬æ•°é‡: {len(self.enhanced_data)}")
                        print(f"   å½“å‰é€‰æ‹©æ ·æœ¬ID: {sample.get('image_id', 'unknown')}")
                        print(f"   å½“å‰æ ·æœ¬è´¨é‡è¯„åˆ†: {sample.get('quality_score', 'N/A'):.4f}")
                        print(f"   è™šæ‹ŸLoss: {loss_virtual_sample:.6f}")
                        print(f"   åŠ æƒåLoss: {virtual_view_loss_to_add:.6f}")
                        print(f"   æƒé‡: {cfg.virtual_view_loss_weight}")
                        if cfg.enable_depth_smooth_loss:
                            print(f"   ğŸ” æ·±åº¦å¹³æ»‘æŸå¤±å·²å¯ç”¨ï¼Œæƒé‡: {cfg.depth_smooth_lambda}")
                    

            else:
                # è™šæ‹Ÿè§†è§’è®­ç»ƒå¼€å§‹æ­¥æ•°ä¹‹å‰ï¼Œè™šæ‹Ÿè§†è§’Lossä¸º0
                virtual_view_loss_to_add = 0.0
                
           

            if cfg.use_bilateral_grid:
                grid_y, grid_x = torch.meshgrid(
                    (torch.arange(height, device=self.device) + 0.5) / height,
                    (torch.arange(width, device=self.device) + 0.5) / width,
                    indexing="ij",
                )
                grid_xy = torch.stack([grid_x, grid_y], dim=-1).unsqueeze(0)
                colors = slice(self.bil_grids, grid_xy, colors, image_ids)["rgb"]


            self.cfg.strategy.step_pre_backward(
                params=self.splats,
                optimizers=self.optimizers,
                state=self.strategy_state,
                step=step,
                info=info,
            )

            # loss
            l1loss = F.l1_loss(colors, pixels)
            if self.cfg.fused_ssim:
                ssimloss = 1.0 - self.ssim(colors.permute(0, 3, 1, 2), pixels.permute(0, 3, 1, 2), padding="valid")
            else:
                ssimloss = 1.0 - self.ssim(pixels.permute(0, 3, 1, 2), colors.permute(0, 3, 1, 2))
            loss = l1loss * (1.0 - cfg.ssim_lambda) + ssimloss * cfg.ssim_lambda
            if cfg.depth_loss:
                # query depths from depth map
                
                points = torch.stack(
                    [
                        points[:, :, 0] / (width - 1) * 2 - 1,
                        points[:, :, 1] / (height - 1) * 2 - 1,
                    ],
                    dim=-1,
                )  # normalize to [-1, 1]
                grid = points.unsqueeze(2)  # [1, M, 1, 2]
                depths = F.grid_sample(depths.permute(0, 3, 1, 2), grid, align_corners=True)  # [1, 1, M, 1]
                depths = depths.squeeze(3).squeeze(1)  # [1, M]
                # calculate loss in disparity space
                disp = torch.where(depths > 0.0, 1.0 / depths, torch.zeros_like(depths))
                disp_gt = 1.0 / depths_gt  # [1, M]
                depthloss = F.l1_loss(disp, disp_gt) * self.scene_scale
                loss += depthloss * cfg.depth_lambda

            if cfg.use_bilateral_grid:
                tvloss = 10 * total_variation_loss(self.bil_grids.grids)
                loss += tvloss

            if cfg.enable_mcmc_opacity_reg:
                loss = loss + cfg.opacity_reg * torch.abs(torch.sigmoid(self.splats["opacities"])).mean()

            if cfg.enable_mcmc_scale_reg:
                loss = loss + cfg.scale_reg * torch.abs(torch.exp(self.splats["scales"])).mean()

            if cfg.enable_phys_scale_reg and step % 10 == 0:
                scale_exp = torch.exp(self.splats["scales"])
                scale_reg = (
                    torch.maximum(
                        scale_exp.amax(dim=-1) / scale_exp.amin(dim=-1),
                        torch.tensor(cfg.max_gauss_ratio),
                    )
                    - cfg.max_gauss_ratio
                )
                scale_reg = 0.1 * scale_reg.mean()
                loss += scale_reg

            # ğŸ¯ æ·»åŠ æ·±åº¦å¹³æ»‘æŸå¤±
            if cfg.enable_depth_smooth_loss and step >= 25000 and depths is not None:
                depth_smooth_loss_value = depth_smooth_loss_4neighbor(depths)
                loss += depth_smooth_loss_value * cfg.depth_smooth_lambda
                if step % 100 == 0:  # æ¯100æ­¥æ‰“å°ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
                    print(f"ğŸ” æ·±åº¦å¹³æ»‘æŸå¤±æƒé‡: {cfg.depth_smooth_lambda}, åŠ æƒåæŸå¤±: {depth_smooth_loss_value * cfg.depth_smooth_lambda:.6f}")

            # ğŸ¯ å…³é”®ï¼šåœ¨æ‰€æœ‰lossè®¡ç®—å®Œæˆåï¼Œæ·»åŠ è™šæ‹Ÿè§†è§’Loss
            loss += virtual_view_loss_to_add
            
            # å¦‚æœå¯ç”¨äº†è™šæ‹Ÿè§†è§’è®­ç»ƒï¼Œæ‰“å°æ€»Lossä¿¡æ¯
            if virtual_view_loss_to_add > 0:
                print(f"ğŸ¯ æœ€ç»ˆLoss: åŸºç¡€={loss.item() - virtual_view_loss_to_add:.4f}, è™šæ‹Ÿè§†è§’={virtual_view_loss_to_add:.4f}, æ€»è®¡={loss.item():.4f}")

            loss.backward()

            desc = f"loss={loss.item():.3f}| " f"sh degree={sh_degree_to_use}| "
            if cfg.depth_loss:
                desc += f"depth loss={depthloss.item():.6f}| "
            if cfg.enable_depth_smooth_loss and depth_smooth_loss_value > 0:
                desc += f"depth smooth={depth_smooth_loss_value.item():.6f}| "
            pbar.set_description(desc)

            # write images (gt and render)
            # if world_rank == 0 and step % 800 == 0:
            #     canvas = torch.cat([pixels, colors], dim=2).detach().cpu().numpy()
            #     canvas = canvas.reshape(-1, *canvas.shape[2:])
            #     imageio.imwrite(
            #         f"{self.render_dir}/train_rank{self.world_rank}.png",
            #         (canvas * 255).astype(np.uint8),
            #     )

            if world_rank == 0 and cfg.tb_every > 0 and step % cfg.tb_every == 0:
                mem = torch.cuda.max_memory_allocated() / 1024**3
                self.writer.add_scalar("train/loss", loss.item(), step)
                self.writer.add_scalar("train/l1loss", l1loss.item(), step)
                self.writer.add_scalar("train/ssimloss", ssimloss.item(), step)
                self.writer.add_scalar("train/num_GS", len(self.splats["means"]), step)
                self.writer.add_scalar("train/mem", mem, step)

                # monitor camera pose optimization
                metrics_dict = {}
                # å¤„ç†DDPåŒ…è£…çš„æƒ…å†µ
                camera_optimizer = self.camera_optimizer.module if hasattr(self.camera_optimizer, 'module') else self.camera_optimizer
                camera_optimizer.get_metrics_dict(metrics_dict)
                for k, v in metrics_dict.items():
                    self.writer.add_scalar(f"train/{k}", v, step)

                # monitor pose learning rate
                self.writer.add_scalar("train/poseLR", pose_scheduler.get_last_lr()[0], step)

                # monitor ATE
                #     self.visualize_traj(step)

                if cfg.depth_loss:
                    self.writer.add_scalar("train/depthloss", depthloss.item(), step)
                if cfg.enable_depth_smooth_loss and depth_smooth_loss_value > 0:
                    self.writer.add_scalar("train/depth_smooth_loss", depth_smooth_loss_value.item(), step)
                if cfg.use_bilateral_grid:
                    self.writer.add_scalar("train/tvloss", tvloss.item(), step)
                if cfg.tb_save_image:
                    canvas = torch.cat([pixels, colors], dim=2).detach().cpu().numpy()
                    canvas = canvas.reshape(-1, *canvas.shape[2:])
                    self.writer.add_image("train/render", canvas, step)
                self.writer.flush()

            # save checkpoint before updating the model
            if step in [i - 1 for i in cfg.save_steps] or step == max_steps - 1:
                mem = torch.cuda.max_memory_allocated() / 1024**3
                stats = {
                    "mem": mem,
                    "ellipse_time": time.time() - global_tic,
                    "num_GS": len(self.splats["means"]),
                }
                print("Step: ", step, stats)
                with open(
                    f"{self.stats_dir}/train_step{step:04d}_rank{self.world_rank}.json",
                    "w",
                ) as f:
                    json.dump(stats, f)
                data = {"step": step, "splats": self.splats.state_dict()}
                if world_size > 1:
                    data["camera_opt"] = self.camera_optimizer.module.state_dict()
                else:
                    data["camera_opt"] = self.camera_optimizer.state_dict()
                if cfg.app_opt:
                    if world_size > 1:
                        data["app_module"] = self.app_module.module.state_dict()
                    else:
                        data["app_module"] = self.app_module.state_dict()
                torch.save(data, f"{self.ckpt_dir}/ckpt_{step}_rank{self.world_rank}.pt")

            if isinstance(self.cfg.strategy, DefaultStrategy):
                self.cfg.strategy.step_post_backward(
                    params=self.splats,
                    optimizers=self.optimizers,
                    state=self.strategy_state,
                    step=step,
                    info=info,
                    packed=cfg.packed,
                )
            elif isinstance(self.cfg.strategy, MCMCStrategy):
                self.cfg.strategy.step_post_backward(
                    params=self.splats,
                    optimizers=self.optimizers,
                    state=self.strategy_state,
                    step=step,
                    info=info,
                    lr=schedulers[0].get_last_lr()[0],
                )
            else:
                assert_never(self.cfg.strategy)

            # Turn Gradients into Sparse Tensor before running optimizer
            if cfg.sparse_grad:
                assert cfg.packed, "Sparse gradients only work with packed mode."
                gaussian_ids = info["gaussian_ids"]
                for k in self.splats.keys():
                    grad = self.splats[k].grad
                    if grad is None or grad.is_sparse:
                        continue
                    self.splats[k].grad = torch.sparse_coo_tensor(
                        indices=gaussian_ids[None],  # [1, nnz]
                        values=grad[gaussian_ids],  # [nnz, ...]
                        size=self.splats[k].size(),  # [N, ...]
                        is_coalesced=len(Ks) == 1,
                    )

            # optimize
            for optimizer in self.optimizers.values():
                optimizer.step()
                optimizer.zero_grad(set_to_none=True)
            for optimizer in self.pose_optimizers:
                if step % cfg.pose_gradient_accumulation_steps == cfg.pose_gradient_accumulation_steps - 1:
                    optimizer.step()
                if step % cfg.pose_gradient_accumulation_steps == 0:
                    optimizer.zero_grad(set_to_none=True)
            for optimizer in self.app_optimizers:
                optimizer.step()
                optimizer.zero_grad(set_to_none=True)
            for optimizer in self.bil_grid_optimizers:
                optimizer.step()
                optimizer.zero_grad(set_to_none=True)
            for scheduler in schedulers:
                scheduler.step()

            # eval the full set
            if step in [i - 1 for i in cfg.eval_steps]:
                if cfg.deblur_eval_enable_during_training and self.testset is not None:
                    if cfg.deblur_eval_enable_pose_opt:
                        self.eval_with_pose_opt(step, "deblur", self.testset)
                    else:
                        self.eval_deblur(step, "deblur", self.testset)
                if cfg.nvs_eval_enable_during_training and self.valset is not None:
                    self.eval_with_pose_opt(step, "nvs", self.valset)
                self.render_traj(step)

            if not cfg.disable_viewer:
                self.viewer.lock.release()
                num_train_steps_per_sec = 1.0 / (time.time() - tic)
                num_train_rays_per_sec = num_train_rays_per_step * num_train_steps_per_sec
                # Update the viewer state.
                self.viewer.state.num_train_rays_per_sec = num_train_rays_per_sec
                # Update the scene.
                self.viewer.update(step, num_train_rays_per_step)

        print(f"è®­ç»ƒå®Œæˆ. æ€»æ—¶é—´: {time.time() - global_tic:.2f} seconds")
        
        # ğŸ“Š è®­ç»ƒç»“æŸåç”Ÿæˆç›¸æœºåˆ†å¸ƒå¯è§†åŒ–
        if world_rank == 0:
            print("ğŸ“Š ç”Ÿæˆè®­ç»ƒç›¸æœºå’ŒDiFix3Dè™šæ‹Ÿç›¸æœºåˆ†å¸ƒå›¾...")
            print(f"   è®­ç»ƒç›¸æœºæ•°é‡: {len(self.all_train_cameras) if self.all_train_cameras is not None else 0}")
            print(f"   DiFix3Dè™šæ‹Ÿç›¸æœºæ‰¹æ¬¡æ•°é‡: {len(self.virtual_camera_batches)}")
            total_virtual_cameras = sum(len(batch) for batch in self.virtual_camera_batches)
            print(f"   DiFix3Dè™šæ‹Ÿç›¸æœºæ€»æ•°é‡: {total_virtual_cameras}")
            total_cameras = (len(self.all_train_cameras) if self.all_train_cameras is not None else 0) + total_virtual_cameras
            print(f"   ç›¸æœºæ€»æ•°: {total_cameras}")
            
            # è¯¦ç»†åˆ†æDiFix3Dè™šæ‹Ÿç›¸æœº
            if self.virtual_camera_batches:
                print("ğŸ“Š DiFix3Dè™šæ‹Ÿç›¸æœºè¯¦ç»†åˆ†æ:")
                for i, batch in enumerate(self.virtual_camera_batches):
                    print(f"   æ‰¹æ¬¡ {i+1}: {len(batch)} ä¸ªDiFix3Dè™šæ‹Ÿç›¸æœº")
                    print(f"     -> DiFix3Dæ¸è¿›å¼æ’å€¼è™šæ‹Ÿè§†è§’")
                    # æ˜¾ç¤ºæ¯ä¸ªæ‰¹æ¬¡çš„ç›¸æœºä½ç½®èŒƒå›´
                    if len(batch) > 0:
                        positions = batch[:, :3, 3].cpu().numpy()
                        print(f"       ä½ç½®èŒƒå›´: X=[{positions[:, 0].min():.3f}, {positions[:, 0].max():.3f}], Y=[{positions[:, 1].min():.3f}, {positions[:, 1].max():.3f}], Z=[{positions[:, 2].min():.3f}, {positions[:, 2].max():.3f}]")
            else:
                print("âš ï¸ æ²¡æœ‰DiFix3Dè™šæ‹Ÿç›¸æœºæ•°æ®")
            
            # ä¿å­˜æœ€ç»ˆçš„è´¨é‡è¯„åˆ†æ•°æ®
            if self.difix3d_processor is not None:
                self.difix3d_processor.save_quality_scores_to_json(step=max_steps-1, result_dir=self.result_dir)


    @torch.no_grad()
    def eval_deblur(self, step: int, stage: str, dataset: Dataset):
        """Entry for evaluation."""
        print("Running evaluation...")
        cfg = self.cfg
        device = self.device
        world_rank = self.world_rank
        world_size = self.world_size

        testloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)
        ellipse_time = 0
        metrics = defaultdict(list)
        for i, data in enumerate(testloader):
            camtoworlds = data["camtoworld"].to(device)
            Ks = data["K"].to(device)
            pixels = data["image"].to(device) / 255.0
            height, width = pixels.shape[1:3]
            image_ids = data["image_id"].to(device)

            # Apply learned mid-virtual-view pose optimizations
            # å¤„ç†DDPåŒ…è£…çš„æƒ…å†µ
            camera_optimizer = self.camera_optimizer.module if hasattr(self.camera_optimizer, 'module') else self.camera_optimizer
            camtoworlds = camera_optimizer.apply_to_cameras(camtoworlds, image_ids, "mid")

            torch.cuda.synchronize()
            tic = time.time()
            colors, _, _ = self.rasterize_splats(
                camtoworlds=camtoworlds,
                Ks=Ks,
                width=width,
                height=height,
                sh_degree=cfg.sh_degree,
                near_plane=cfg.near_plane,
                far_plane=cfg.far_plane,
            )  # [1, H, W, 3]
            colors = torch.clamp(colors, 0.0, 1.0)
            torch.cuda.synchronize()
            ellipse_time += time.time() - tic

            if world_rank == 0:
                # write images
                canvas = torch.cat([pixels, colors], dim=2).squeeze(0).cpu().numpy()
                imageio.imwrite(f"{self.render_dir}/{step:04d}_{stage}_{i:04d}.png", (canvas * 255).astype(np.uint8))

                pixels_p = pixels.permute(0, 3, 1, 2)  # [1, 3, H, W]
                colors_p = colors.permute(0, 3, 1, 2)  # [1, 3, H, W]
                metrics["psnr"].append(self.psnr(colors_p, pixels_p))
                metrics["ssim"].append(self.ssim(colors_p, pixels_p))
                metrics["lpips"].append(self.lpips(colors_p, pixels_p))
                if cfg.use_bilateral_grid:
                    cc_colors = color_correct(colors, pixels)
                    cc_colors_p = cc_colors.permute(0, 3, 1, 2)  # [1, 3, H, W]
                    metrics["cc_psnr"].append(self.psnr(cc_colors_p, pixels_p))
                    metrics["cc_ssim"].append(self.ssim(cc_colors_p, pixels_p))
                    metrics["cc_lpips"].append(self.lpips(cc_colors_p, pixels_p))
                    # write images
                    canvas = torch.cat([pixels, cc_colors], dim=2).squeeze(0).cpu().numpy()
                    imageio.imwrite(
                        f"{self.render_dir}/{step:04d}_{stage}_{i:04d}_corrected.png", (canvas * 255).astype(np.uint8)
                    )

        if world_rank == 0:
            ellipse_time /= len(testloader)

            stats = {k: torch.stack(v).mean().item() for k, v in metrics.items()}
            
            # æ·»åŠ æœ€ä½³ç»“æœç»Ÿè®¡ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
            best_stats = {}
            for k, v in metrics.items():
                if "psnr" in k or "ssim" in k:
                    best_stats[f"best_{k}"] = torch.stack(v).max().item()
                elif "lpips" in k:
                    best_stats[f"best_{k}"] = torch.stack(v).min().item()
            
            # æ·»åŠ æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†ç»“æœï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
            detailed_results = {}
            for k, v in metrics.items():
                detailed_results[f"{k}_per_sample"] = [float(val.item()) for val in v]
            
            stats.update(
                {
                    "ellipse_time": ellipse_time,
                    "num_GS": len(self.splats["means"]),
                }
            )
            
            # åˆå¹¶æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯
            final_stats = {**stats, **best_stats, **detailed_results}
            
            print(
                f"PSNR: {stats['psnr']:.3f}, SSIM: {stats['ssim']:.4f}, LPIPS: {stats['lpips']:.3f} "
                f"Time: {stats['ellipse_time']:.3f}s/image "
                f"Number of GS: {stats['num_GS']}"
            )
            # æ‰“å°æœ€ä½³ç»“æœï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
           
            
            # save stats as jsonï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼Œä½†å¢åŠ æ›´å¤šä¿¡æ¯ï¼‰
            with open(f"{self.stats_dir}/{stage}_step{step:04d}.json", "w") as f:
                json.dump(final_stats, f, indent=2)
            
            # save stats to tensorboardï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰
            for k, v in final_stats.items():
                if not k.endswith("_per_sample"):  # åªä¿å­˜æ±‡æ€»æŒ‡æ ‡åˆ°tensorboard
                    self.writer.add_scalar(f"{stage}/{k}", v, step)
            self.writer.flush()

    def eval_with_pose_opt(self, step: int, stage: str, dataset: Dataset):
        """Entry for evaluation."""
        print("Running evaluation...")
        cfg = self.cfg
        device = self.device

        valloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)

        # Freeze the scene
        for optimizer in self.optimizers.values():
            for param_group in optimizer.param_groups:
                param_group["params"][0].requires_grad = False

        metrics = defaultdict(list)
        for i, data in enumerate(valloader):
            camtoworlds = data["camtoworld"].to(device)
            Ks = data["K"].to(device)
            pixels = data["image"].to(device) / 255.0  # [1, H, W, 3]
            height, width = pixels.shape[1:3]
            image_ids = data["image_id"].to(device)

            pixels_p = pixels.permute(0, 3, 1, 2)  # [1, 3, H, W]

            eval_pose_adjust = CameraOptModuleSE3(1).to(self.device)
            eval_pose_adjust.random_init(cfg.pose_noise)
            eval_pose_optimizer = torch.optim.Adam(
                eval_pose_adjust.parameters(),
                lr=cfg.nvs_pose_lr * math.sqrt(cfg.batch_size),
                weight_decay=cfg.nvs_pose_reg,
                eps=1e-15,
            )

            scheduler = torch.optim.lr_scheduler.ExponentialLR(
                eval_pose_optimizer, gamma=cfg.pose_opt_lr_decay ** (1.0 / cfg.max_steps)
            )

            NVS_STEPS = cfg.nvs_steps_final if step == cfg.max_steps - 1 else cfg.nvs_steps
            for j in range(NVS_STEPS):
                camtoworlds_new = eval_pose_adjust(camtoworlds, torch.tensor([0]).to(self.device))
                colors, alphas, info = self.rasterize_splats(
                    camtoworlds=camtoworlds_new,
                    Ks=Ks,
                    width=width,
                    height=height,
                    sh_degree=cfg.sh_degree,
                    near_plane=cfg.near_plane,
                    far_plane=cfg.far_plane,
                    image_ids=image_ids,
                    render_mode="RGB",
                )
                # clamping here should be fine since we are only optimizing the camera
                colors = torch.clamp(colors, 0.0, 1.0)
                colors_p = colors.permute(0, 3, 1, 2).detach()  # [1, 3, H, W]

                # loss
                l1loss = F.l1_loss(colors, pixels)
                loss = l1loss

                loss.backward()

                eval_pose_optimizer.step()
                eval_pose_optimizer.zero_grad(set_to_none=True)

                scheduler.step()
                with torch.no_grad():
                    if j % 20 == 0:
                        psnr = self.psnr(colors_p, pixels_p)
                        ssim = self.ssim(colors_p, pixels_p)
                        lpips = self.lpips(colors_p, pixels_p)
                        print(
                            f"Stage {stage} at Step_{step:04d}:"
                            f"NVS_IMG_#{i:04d}_step_{j:04d}:"
                            f"PSNR: {psnr.item():.3f}, SSIM: {ssim.item():.4f}, LPIPS: {lpips.item():.3f} "
                        )
                        if cfg.use_bilateral_grid:
                            cc_colors = color_correct(colors, pixels)
                            cc_colors_p = cc_colors.permute(0, 3, 1, 2)
                            cc_psnr = self.psnr(cc_colors_p, pixels_p)
                            cc_ssim = self.ssim(cc_colors_p, pixels_p)
                            cc_lpips = self.lpips(cc_colors_p, pixels_p)
                            print(
                                f"Corrected PSNR: {cc_psnr.item():.3f}, SSIM: {cc_ssim.item():.4f}, LPIPS: {cc_lpips.item():.3f} "
                            )
                        # # NVS Debugging
                        # stats = {
                        #     "psnr": psnr.item(),
                        #     "ssim": ssim.item(),
                        #     "lpips": lpips.item(),
                        # }
                        # for k, v in stats.items():
                        #     self.writer.add_scalar(f"nvs/{step}/{i}/{k}", v, j)
                        # self.writer.add_scalar(f"{stage}/{step}/{i}/pose_lr", scheduler.get_last_lr()[0], j)
                        # self.writer.add_scalar(f"{stage}/{step}/{i}/camera_opt_translation", eval_pose_adjust.poses_opt[:, :3].mean(), j)
                        # self.writer.add_scalar(f"{stage}/{step}/{i}/camera_opt_rotation", eval_pose_adjust.poses_opt[:, 3:].mean(), j)
                        # self.writer.flush()
            metrics["psnr"].append(psnr)
            metrics["ssim"].append(ssim)
            metrics["lpips"].append(lpips)
            if cfg.use_bilateral_grid:
                metrics["cc_psnr"].append(cc_psnr)
                metrics["cc_ssim"].append(cc_ssim)
                metrics["cc_lpips"].append(cc_lpips)
            
            # write images
            canvas = torch.cat([pixels, colors], dim=2).squeeze(0).detach().cpu().numpy()
            imageio.imwrite(
                f"{self.render_dir}/{step:04d}_{stage}_{i:04d}_{j:04d}.png", (canvas * 255).astype(np.uint8)
            )
            if cfg.use_bilateral_grid:
                canvas = torch.cat([pixels, cc_colors], dim=2).squeeze(0).detach().cpu().numpy()
                imageio.imwrite(
                    f"{self.render_dir}/{step:04d}_{stage}_{i:04d}_{j:04d}_corrected.png",
                    (canvas * 255).astype(np.uint8),
                )
        # è®¡ç®—å¹³å‡å€¼
        stats = {k: torch.stack(v).mean().item() for k, v in metrics.items()}
        
        # è®¡ç®—æœ€ä½³å€¼ï¼ˆå¯¹äºPSNRå’ŒSSIMæ˜¯æœ€å¤§å€¼ï¼Œå¯¹äºLPIPSæ˜¯æœ€å°å€¼ï¼‰
        best_stats = {}
        for k, v in metrics.items():
            if "psnr" in k or "ssim" in k:
                best_stats[f"best_{k}"] = torch.stack(v).max().item()
            elif "lpips" in k:
                best_stats[f"best_{k}"] = torch.stack(v).min().item()
        
        # ä¿å­˜æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†ç»“æœ
        detailed_results = {}
        for k, v in metrics.items():
            detailed_results[f"{k}_per_sample"] = [float(val.item()) for val in v]
        
        # åˆå¹¶æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯
        final_stats = {**stats, **best_stats, **detailed_results}
        
        # æ‰“å°æœ€ä½³ç»“æœ
        print(f"Best PSNR: {best_stats['best_psnr']:.3f}, Best SSIM: {best_stats['best_ssim']:.4f}, Best LPIPS: {best_stats['best_lpips']:.3f}")
        if cfg.use_bilateral_grid:
            print(f"Best Corrected PSNR: {best_stats['best_cc_psnr']:.3f}, Best Corrected SSIM: {best_stats['best_cc_ssim']:.4f}, Best Corrected LPIPS: {best_stats['best_cc_lpips']:.3f}")
        
        # save stats as json
        with open(f"{self.stats_dir}/{stage}_step{step:04d}.json", "w") as f:
            json.dump(final_stats, f, indent=2)

        # save stats to tensorboard
        for k, v in final_stats.items():
            if not k.endswith("_per_sample"):  # åªä¿å­˜æ±‡æ€»æŒ‡æ ‡åˆ°tensorboard
                self.writer.add_scalar(f"{stage}/{k}", v, step)
        self.writer.flush()

        # Unfreeze the scene
        for optimizer in self.optimizers.values():
            for param_group in optimizer.param_groups:
                param_group["params"][0].requires_grad = True

    @torch.no_grad()
    def eval_traj(self, step: int):
        # TODO: add gt trajectory

        # Get estimated trajectory
        # å¤„ç†DDPåŒ…è£…çš„æƒ…å†µ
        camera_optimizer = self.camera_optimizer.module if hasattr(self.camera_optimizer, 'module') else self.camera_optimizer
        camtoworlds = camera_optimizer.get_cameras()

        raise NotImplementedError

    def _init_viewer_state(self) -> None:
        """Initializes viewer scene with given train dataset"""
        if not self.cfg.disable_viewer and isinstance(self.viewer, PoseViewer):
            assert self.viewer and self.trainset
            self.viewer.init_scene(train_dataset=self.trainset, train_state="training")


def main(local_rank: int, world_rank, world_size: int, cfg: DeblurDiFix3DConfig):
    if world_size > 1 and not cfg.disable_viewer:
        cfg.disable_viewer = True
        if world_size > 1:
            print("Viewer is disabled in distributed training.")

    runner = DeblurDiFix3DRunner(local_rank, world_rank, world_size, cfg)

    if cfg.ckpt is not None:
        # run eval only
        ckpts = [torch.load(file, map_location=runner.device, weights_only=False) for file in cfg.ckpt]
        for k in runner.splats.keys():
            runner.splats[k].data = torch.cat([ckpt["splats"][k].detach().to(runner.device) for ckpt in ckpts])
        runner.camera_optimizer.load_state_dict(ckpts[0]["camera_opt"])
        step = ckpts[0]["step"]
        if runner.testset is not None:
            if cfg.deblur_eval_enable_pose_opt:
                runner.eval_with_pose_opt(step=step, stage="deblur", dataset=runner.testset)
            else:
                runner.eval_deblur(step=step, stage="deblur", dataset=runner.testset)
        if runner.valset is not None:
            runner.eval_with_pose_opt(step=step, stage="nvs", dataset=runner.valset)

        runner.render_traj(step=step)
    else:
        runner.train()

    if not cfg.disable_viewer:
        print("Viewer running... Ctrl+C to exit.")
        time.sleep(1000000)


if __name__ == "__main__":
    """
    Usage:
    ```bash
    # Single GPU training
    CUDA_VISIBLE_DEVICES=0 python simple_trainer.py default
    # Distributed training on 4 GPUs: Effectively 4x batch size so run 4x less steps.
    CUDA_VISIBLE_DEVICES=0,1,2,3 python simple_trainer.py default --steps_scaler 0.25
    """

    # Config objects we can choose between.
    # Each is a tuple of (CLI description, config object).
    configs = {
        "default": (
            "Gaussian splatting training using densification heuristics from the original paper.",
            DeblurDiFix3DConfig(
                strategy=DefaultStrategy(
                    verbose=True,
                    grow_grad2d=3e-3,
                    absgrad=True,
                    refine_start_iter=1000,
                ),
            ),
        ),
        "mcmc": (
            "Gaussian splatting training using densification from the paper '3D Gaussian Splatting as Markov Chain Monte Carlo'.",
            DeblurDiFix3DConfig(
                init_opa=0.5,
                init_scale=0.1,
                strategy=MCMCStrategy(verbose=True, cap_max=500_000),
            ),
        ),
    }

    cfg = tyro.extras.overridable_config_cli(configs)
    cfg.adjust_steps(cfg.steps_scaler)
    cli(main, cfg, verbose=True)
